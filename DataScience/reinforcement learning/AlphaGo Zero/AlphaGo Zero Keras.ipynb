{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaGo Zero for connect 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://web.stanford.edu/~surag/posts/alphazero.html <br>\n",
    "https://www.youtube.com/watch?v=62nq4Zsn8vc&ab_channel=JoshVarty <br>\n",
    "https://github.com/JoshVarty/AlphaZeroSimple <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value network and self play\n",
    "1) Play a bunch of games against yourself (using policy network to decide where to move) and record whether a given board state eventually lead to a win or a loss. We take a snapshot of what the board looks like when presented to each player and want to determine how good of a position they are currently in before they take a move. <br>\n",
    "\n",
    "`\n",
    " BOARD STATE \n",
    "[ 0  0  0  0] # Start of the game, presented to Player 1. Player 1 plays in the first position.\n",
    "[-1  0  0  0] # Player 2 is presented with this board. Player 2 plays in the third position.\n",
    "[ 1  0 -1  0] # Player 1 is presented with this board. Player 1 plays in the second position\n",
    "[-1 -1  1  0] # Player 2 is presented with this board. Player 2 was lost as Player 1 can connect 2\n",
    "`\n",
    "\n",
    "2) Create a labelled training set for the value NN by going back through all of the game states held by Player 1 and marking them with a reward of 1. Similarily go through all of the states held by Player 2 and mark them with a reward of -1. <br>\n",
    "\n",
    "` \n",
    "  BOARD STATE   RES \n",
    "([ 0  0  0  0], 1) # Player 1 \n",
    "([-1  0  0  0],-1) # Player 2 \n",
    "([ 1  0 -1  0], 1) # Player 1 \n",
    "([-1 -1  1  0],-1) # Player 2 \n",
    "`\n",
    "\n",
    "3) After completing thousands of self play games, we shuffle up the board training data and feed it into our neural network to train it to recognise what the probability of winning is given a particular board state. The output will be from -1 to 1. This is essentially an image recognition task, which is why AlphaGo uses resnets.\n",
    "\n",
    "<img src=\"value_network.PNG\" alt=\"drawing\" width=\"400\" align='left'/> \n",
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "### Policy Network\n",
    "Takes a game state / board as input and outputs next move a probability distribution of next moves, with move with the highest probability being the move which the policy network thinks will do the best.\n",
    "\n",
    "<img src=\"policy_network.PNG\" alt=\"drawing\" width=\"400\" align='left'/> \n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "For example, based upon the output above, the network thinkgs we will win 88% of the time if you make a move into the first position and have a 10% probability of winning in the last position. Wierdly, it outputs a non-zero chance of winning if we take invalid moves where there are already pieces. This is because we cannot control the NN to output 0 values for invalid moves. Instead, we will have to write code to mask out invalid moves and redistribute the values.\n",
    "\n",
    "The way we train the network is to actually encourage it to give the same output as the Monte Carlo Tree search (more on that later, it actually uses the relative count of how often a particular state gets visited when conducting MCTS). To do this we create a dataset that includes a board state, and also what the MCTS suggested we do at that position. <br>\n",
    "\n",
    "`\n",
    "BOARD STATE             MCTS        \n",
    "([ 0  0  0  0], [0.1, 0.4, 0.4, 0.1]) \n",
    "([-1  0  0  0], [0.0, 0.3, 0.3, 0.3]) \n",
    "([ 1  0 -1  0], [0.0, 0.8, 0.0, 0.2]) \n",
    "`\n",
    "\n",
    "There is actually an interesting circular dependency, where by we are training our policy network to output results that match the MCTS, but we actually use our policy to network to choose the most promising next positions to expolore with MCTS. In theory, improving the policy network will improve the MCTS and visa versa. <br>\n",
    "\n",
    "\n",
    "### AlphaGo Zero actually uses the same network to output the policy and value\n",
    "\n",
    "<img src=\"policy_value.PNG\" alt=\"drawing\" width=\"400\" align='left'/>\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### MCTS\n",
    "Normally MCTS would entail doing rollouts until the a particular game is ran to conclusion and then the result is backed up to update the previous states; however, with AlphaGo Zero we actually use the results from the value network to score each state (called \"virtual losses\"). Of course, if we do happen to reach a terminal game state, then the actual result will be used instead and backed up. The lack of focus of complete rollouts can be hugely befefitial in a game like Go where there are any moves until termination.\n",
    "\n",
    "How it works: <br>\n",
    "1) We feed the root node (starting game state) `[0 0 0 0]` into our combined value/policy network at get its value and next move probabilities. Given a randomly initialised NN to start with, lets say it outputs: <br>\n",
    "`0` for our value which remember is between -1, 1. <br>\n",
    "`[0.25 0.25 0.25 0.25]` for the action probabilities. <br>\n",
    "\n",
    "2) We calculate a score called **UCB (upper confidence bound)** for each child node, and pick the one with the highest. The UCB takes into account: <br>\n",
    "- Child node visit counts vs other children at this level.\n",
    "- Policy network probabilities for child state.\n",
    "- Value network values for child state.\n",
    "\n",
    "For our first action / step the UCB components are:\n",
    "- The visit counts of each child node (next action) will clearly be zero.\n",
    "- Action probabilites given by our randomly initialised NN were equal for each child. \n",
    "- Because we havn't visited any child nodes yet, we don't have a value network score for them, so the values are assumed to be 0 (remember, its score from -1, 1).\n",
    "\n",
    "<ins>Upper Confidence Bound (UCB)<ins><br>\n",
    "    \n",
    "<img src=\"UCB.PNG\" alt=\"drawing\" width=\"550\" align='left'/>\n",
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "UCB score for each child node: <br>\n",
    "UCB(s,a) = -0 + 0.25 ∗ sqrt(0) / (1 + 0) = **0** <br>\n",
    "\n",
    "3) Given all child nodes have an equal UCB score, we pick one at random (the first one). <br>\n",
    "We then evaluate the state `[-1 0 0 0]` (from the perspective of player 2 now) by passing it to the NN and increasing the `visit count` of this node (action) by one. <br>\n",
    "**The NN outputs:** <br>\n",
    "`value: 0.5` <br>\n",
    "`proabilities: [0.33 0.33 0.33]` for the three remaining valid moves. <br>\n",
    "\n",
    "<img src=\"nodes1.PNG\" alt=\"drawing\" width=\"800\" align='left'/> \n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "4) We `backup` the `0.5` value of this child node to the root node, where the value of the root becomes the **average** of its visited child nodes, but its **negative**, because its from the perspective of the other player, so the root node value becomes `-0.5`. <br>\n",
    "\n",
    "5) We calculate the UCB score for each 1st level state to decide what to do next: <br>\n",
    "Node1 UCB = -0.5 + 0.25 * sqrt(1) / 2 = -0.5 + 0.125 = **-0.375** <br>\n",
    "Node2,3,4 UCB = 0 + 0 * sqrt(1) / 0 = **0** <br>\n",
    "\n",
    "Given that the first node we visited didnt give us a positive value (with respect to the root/parent node) it makes sense for us to explore other nodes. In this example we set our NN to ouput the same value for each state (because it's randomly initialized and not trained yet). <br>\n",
    "<img src=\"nodes2.PNG\" alt=\"drawing\" width=\"800\" align='left'/> \n",
    "<br><br><br><br><br><br><br>\n",
    "\n",
    "6) This leads us to take another random action (the first node). Because this actions board state has already been evaluated by our NN, we expand its node and look for the next possible moves. This leads to three valid moves we could take. <br>\n",
    "Given that none of them been visited yet, they have a value of 0, equal prior probabilities and therefore equal UCB scores. We then pick the one at random (the first one) and evaluate it with our NN.<br>\n",
    "We evaluate the state `[1 -1 0 0]` (from the perspective of player 2 now) by passing it to the NN and increasing the `visit count` of this node (action) by one. <br>\n",
    "**The NN outputs:** <br>\n",
    "`value: 0.5` <br>\n",
    "`proabilities: [0.5 0.5]` for the two remaining valid moves. <br>\n",
    "\n",
    "<img src=\"nodes.PNG\" alt=\"drawing\" width=\"800\" align='left'/> \n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "7) We `backup` the `0.5` value of this child node to its parent node and ultimately the root node (remebering to alternative negative and positive to account for the perspective of each player). <br>\n",
    "*It is interesting to note that the initial `value` that the NN outputted for a state is used as part of the average for a states final `value`. e.g the state at `[-1 0 0 0]` here gets an averaged value of 0 becase its initial NN value was 0.5, and now its child got a value of -0.5 (from the parents perspective), leaving an average of 0.  <br>\n",
    "\n",
    "8) We repeat the above steps 5-7 for the set number of MCTS simulations (100 as default). We then use the visit counts for the next actions (1st level) to create a probability distrbution. Lets say we have the following visit counts `[60 20 10 10]` we noramlize it to `[0.6 0.2 0.2 0.1]`. <br>\n",
    "This distribution is used to take the best action for our **first move during self play** and also to collected to train the policy NN to output probabilities more inline with MCTS. \n",
    "\n",
    "9) We repeat steps all of the steps 1-8 but this time inputting the new state from self play. This continues until we reach the end of a game of self play (`episode`) and use the actual result to backpropegate label all of the self play states (more info at the start of this doc).\n",
    "\n",
    "10) We play a set number of full self play games / `episodes` (default is 100) and then train the NN. We use the collected value labels and MCTS visit probabilites for each self play state to train the value/policy NN. Both are used as part of the loss function to train both parts of the NN. We pass the shuffled dataset to the NN for a set number of `epochs` (default is 2).\n",
    "\n",
    "11) We repeat the above steps for a number of `training iterations` (default is 500), meaning that the agent will have played 500 * 100 = 5,000 self play games. Each game takes 4 moves to complete, with each move needing 100 MCTS simulations, so that is 500 * 4 * 100 = 20,000 total MCTS simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, format='%(levelname)-8s %(message)s')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "#log.setLevel(logging.DEBUG)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect 2 game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_nn(board):\n",
    "    ''' Feed state into neural network and get value and probability array'''\n",
    "    state_value = 0.5\n",
    "    child_probs = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    return state_value, child_probs\n",
    "\n",
    "\n",
    "def get_valid_moves(board):\n",
    "    mask = board == np.array([0,0,0,0])\n",
    "    valid_moves = np.nonzero(mask)[0]\n",
    "    return valid_moves\n",
    "\n",
    "\n",
    "def clean_nn_probs(board, child_probs):\n",
    "    '''Mask out invalid moves from NN probabilities array'''\n",
    "    mask = board == np.array([0,0,0,0])\n",
    "    masked_probs = child_probs * mask\n",
    "    rescale_probs = masked_probs / np.sum(masked_probs)\n",
    "    return rescale_probs\n",
    "\n",
    "\n",
    "class Node:\n",
    "    '''Create state nodes'''\n",
    "    def __init__(self, player, state=None, prior_prob=0):\n",
    "        self.player = player\n",
    "        self.state = state.copy()\n",
    "        self.prior_prob = prior_prob\n",
    "        self.total_value = 0\n",
    "        self.value = 0\n",
    "        self.visit_count = 0\n",
    "        self.ucb = 0\n",
    "        self.children = {}\n",
    "     \n",
    "    def create_children(self, child_probs, children):\n",
    "        for action in children:\n",
    "            child_state = self.state.copy()\n",
    "            child_state[action] = 1\n",
    "            flip_state = flip_board(child_state)\n",
    "            self.children[action] = Node(prior_prob=child_probs[action],\n",
    "                                         state=flip_state,\n",
    "                                         player=-self.player)\n",
    "        self.update_child_ucbs()\n",
    "                \n",
    "    def update_child_ucbs(self):\n",
    "        '''Calculate ucb score for each child state'''\n",
    "        scores = []\n",
    "        for action, child in self.children.items():\n",
    "            child.ucb = (-child.value + child.prior_prob\n",
    "                        * (np.sqrt(self.visit_count) / (1 + child.visit_count)))\n",
    "            scores.append((child.ucb, action, -child.value, child.prior_prob,\n",
    "                           self.visit_count, child.visit_count))\n",
    "            \n",
    "        sorted_scores = sorted(scores, reverse=True)\n",
    "        log.debug(f'child scores: {sorted_scores}')\n",
    "        self.best_child = sorted_scores[0][1]\n",
    "\n",
    "    \n",
    "    def update_value(self, value):\n",
    "        self.total_value += value\n",
    "        self.visit_count += 1\n",
    "        self.value = self.total_value / self.visit_count\n",
    "\n",
    "            \n",
    "    def backup(self, search_path, reward):\n",
    "        reward_perspective = -1\n",
    "        for node in reversed(search_path):\n",
    "            node.update_value(reward * reward_perspective)\n",
    "            node.update_child_ucbs()\n",
    "            reward_perspective *= -1\n",
    "   \n",
    "\n",
    "def take_move(board, position):\n",
    "    assert board[position] == 0 , 'invalid move, place already taken'\n",
    "    board[position] = 1\n",
    "    return board\n",
    "\n",
    "\n",
    "def flip_board(board):\n",
    "    board = board.copy() * -1\n",
    "    return board\n",
    "\n",
    "\n",
    "def is_loss(board):\n",
    "    total = 0\n",
    "    for board_marker in board:\n",
    "        if board_marker in [0, 1]:\n",
    "            total = 0\n",
    "        else:\n",
    "            total += board_marker\n",
    "        if total == -2:\n",
    "            return True\n",
    "    return False \n",
    "  \n",
    "    \n",
    "def end_game(board, valid_moves):\n",
    "    if is_loss(board): # loss\n",
    "        reward = -1\n",
    "        log.debug('loss')\n",
    "    elif len(valid_moves) == 0: # draw\n",
    "        reward = 0\n",
    "        log.debug('draw')\n",
    "    else:\n",
    "        reward = None\n",
    "    return reward\n",
    "        \n",
    "\n",
    "def MCTS(board, cur_player, num_simulations=100):\n",
    "    root = Node(player=cur_player, state=board)\n",
    "\n",
    "    for _ in range(num_simulations + 1):\n",
    "        log.debug(f'simulation no.: {_}')\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "        reward = 0\n",
    "\n",
    "        while len(node.children) > 0:\n",
    "            log.debug('expand children')\n",
    "            node = node.children[node.best_child]\n",
    "            board = node.state\n",
    "            log.debug(f'board: {board}')\n",
    "            cur_player = node.player\n",
    "            search_path.append(node)\n",
    "\n",
    "        valid_moves = get_valid_moves(board)\n",
    "        log.debug(f'cur player: {cur_player}')\n",
    "        log.debug(f'board: {board}')\n",
    "        log.debug(f'valid moves: {valid_moves}')\n",
    "        reward = end_game(board, valid_moves)\n",
    "        if reward is None: # can move\n",
    "            state_value, child_probs = test_nn(board)\n",
    "            reward = state_value\n",
    "            clean_child_probs = clean_nn_probs(board, child_probs)\n",
    "            node.create_children(child_probs=clean_child_probs, children=valid_moves)\n",
    "\n",
    "        log.debug(f'reward: {reward}')\n",
    "        node.update_value(reward)\n",
    "        node.backup(search_path[:-1], reward)\n",
    "        log.debug('------------')\n",
    "        \n",
    "    return root\n",
    "\n",
    "\n",
    "def calc_mcts_probs(root):\n",
    "    board_template = np.array([0,0,0,0])\n",
    "    for action, node in root.children.items():\n",
    "        board_template[action] = node.visit_count\n",
    "    mcts_probs = board_template / board_template.sum()\n",
    "    return mcts_probs\n",
    "\n",
    "\n",
    "def select_temperature_based_action(root, temperature):\n",
    "    visit_counts = np.array([child.visit_count for child in root.children.values()])\n",
    "    actions = [action for action in root.children.keys()]\n",
    "    if temperature == 0:\n",
    "        action = actions[np.argmax(visit_counts)]\n",
    "    elif temperature == float(\"inf\"):\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # See paper appendix Data Generation\n",
    "        visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "        visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "        action = np.random.choice(actions, p=visit_count_distribution)\n",
    "    return action\n",
    "    \n",
    "    \n",
    "def episode(num_mcts_simulations=1500, temperature=0.1, board=np.array([0,0,0,0])):\n",
    "    cur_player = 1\n",
    "    reward = None\n",
    "    train_examples = []\n",
    "    board = board.copy()\n",
    "    while True:\n",
    "        root = MCTS(board, cur_player=cur_player, num_simulations=num_mcts_simulations)\n",
    "        valid_moves = get_valid_moves(board)\n",
    "        reward = end_game(board, valid_moves)\n",
    "        if reward is None:\n",
    "            mcts_probs = calc_mcts_probs(root)\n",
    "            train_examples.append([board.copy(), mcts_probs.copy()])\n",
    "            action = select_temperature_based_action(root, temperature=temperature)\n",
    "            board = take_move(board, position=action)\n",
    "            board = flip_board(board)\n",
    "            cur_player *= -1\n",
    "        else:\n",
    "            train_examples.append([board, np.array([0,0,0,0])])\n",
    "            return train_examples, reward\n",
    "        \n",
    "\n",
    "def sort_train_data(train_examples, reward):\n",
    "    X = np.vstack([board for board, probs in train_examples])\n",
    "    policy_y = np.vstack([probs for board, probs in train_examples])\n",
    "    value_y = []\n",
    "    for _ in range(len(train_examples)):\n",
    "        value_y.append(reward)\n",
    "        reward *= -1\n",
    "    value_y = np.vstack(list(reversed(value_y)))\n",
    "    return X, policy_y, value_y\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, lr):\n",
    "        self.policy_model, self.value_model = self.build_network(lr=lr)\n",
    "    \n",
    "    \n",
    "    def build_network(self, board_size=4, lr=1e-4):\n",
    "        inputs = Input(shape=(board_size))\n",
    "        dense_1 = Dense(units=100, activation='relu')(inputs)\n",
    "        dense_2 = Dense(units=100, activation='relu')(dense_1)\n",
    "        dense_3 = Dense(units=100, activation='relu')(dense_2)\n",
    "        policy = Dense(units=board_size, activation='softmax')(dense_3)\n",
    "        value = Dense(units=1, activation=None)(dense_3)\n",
    "         \n",
    "\n",
    "        def custom_loss(y_true, y_pred):\n",
    "            return categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "        \n",
    "        policy_model = tf.keras.Model(inputs=inputs, outputs=policy)\n",
    "        policy_model.compile(optimizer=Adam(lr=lr), loss=custom_loss)#, run_eagerly=True) #debug\n",
    "        value_model = tf.keras.Model(inputs=inputs, outputs=value)\n",
    "        value_model.compile(optimizer=Adam(lr=lr), loss='mse')\n",
    "        return policy_model, value_model\n",
    "    \n",
    "    \n",
    "    def learn(self, X, policy_y, value_y, epochs=1, loops=10, verbose=0):\n",
    "        for _ in range(loops):\n",
    "            self.policy_model.fit(X, policy_y, shuffle=False, epochs=epochs, verbose=verbose)\n",
    "            self.value_model.fit(X, value_y, shuffle=False, epochs=epochs, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\n",
      " 20%|████████████████▌                                                                  | 1/5 [02:16<09:05, 136.48s/it]\n",
      " 40%|█████████████████████████████████▏                                                 | 2/5 [04:27<06:44, 134.76s/it]\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [06:38<04:27, 133.60s/it]\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [08:50<02:13, 133.09s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [11:01<00:00, 132.21s/it]\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "train = True\n",
    "#############\n",
    "\n",
    "if train:\n",
    "    num_iterations = 5\n",
    "    num_episodes = 100\n",
    "    mcts_simulations = 1500\n",
    "\n",
    "    alpha_go = Network(lr=1e-5)\n",
    "    for _ in tqdm(range(num_iterations)):\n",
    "        X, policy_y, value_y = [], [], []\n",
    "        for _ in range(num_episodes):\n",
    "            train_examples, reward = episode(num_mcts_simulations=mcts_simulations,\n",
    "                                             temperature=10)\n",
    "            ep_X, ep_policy_y, ep_value_y = sort_train_data(train_examples, reward)\n",
    "            X.extend(ep_X)\n",
    "            policy_y.extend(ep_policy_y)\n",
    "            value_y.extend(ep_value_y)\n",
    "        #print(pd.DataFrame(np.concatenate([X, policy_y, value_y], axis=1)))\n",
    "        alpha_go.learn(np.vstack(X), np.vstack(policy_y), np.vstack(value_y),\n",
    "                       epochs=1, loops=200, verbose=0)\n",
    "    alpha_go.policy_model.save_weights('alpha_go_policy_weights/')  \n",
    "    alpha_go.value_model.save_weights('alpha_go_value_weights/')\n",
    "else:\n",
    "    print('loading pre-trained weights')\n",
    "    alpha_go = Network(lr=1e-5)\n",
    "    alpha_go.policy_model.load_weights('alpha_go_policy_weights/')\n",
    "    alpha_go.value_model.load_weights('alpha_go_value_weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# board = np.array([[0,-1,1,0]])\n",
    "# alpha_go.policy_model.predict(board), alpha_go.value_model.predict(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.options.display.max_rows = 1000\n",
    "# data = pd.DataFrame(np.concatenate([X, policy_y, value_y], axis=1))\n",
    "# data = data[(\n",
    "#     (data[0] == board[0][0])\n",
    "#   & (data[1] == board[0][1])\n",
    "#   & (data[2] == board[0][2])\n",
    "#   & (data[3] == board[0][3]) \n",
    "# )]\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play against alpha go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select difficulty [(easy/hard]e\n",
      "Would you like to start? [(y/n]y\n",
      "--------------------------------------------------------\n",
      "Welcome to a new game of connect 2 against AlphaGo Zero!\n",
      "--------------------------------------------------------\n",
      "\n",
      "The current board looks like this:\n",
      "[0 0 0 0]\n",
      "\n",
      "It is your turn.\n",
      "AlphaGo belives you have a 70% chance of winning from here.\n",
      "\n",
      "Pick a valid board position to move: [0 1 2 3]\n",
      "Your move is: 1\n",
      "------\n",
      "\n",
      "The current board looks like this:\n",
      "[0 1 0 0]\n",
      "\n",
      "AlphaGo Zero's turn\n",
      "AlphaGo Zero belives it has a 15% chance of winning from here.\n",
      "\n",
      "This is what is looks like inside AlphaGo Zero's brain:\n",
      "[0.2418794  0.         0.36305967 0.3950609 ]\n",
      "\n",
      "AlphaGo Zero's move is: 3\n",
      "------\n",
      "\n",
      "\n",
      "The current board looks like this:\n",
      "[ 0  1  0 -1]\n",
      "\n",
      "It is your turn.\n",
      "AlphaGo belives you have a 98% chance of winning from here.\n",
      "\n",
      "Pick a valid board position to move: [0 2]\n",
      "Your move is: 0\n",
      "------\n",
      "\n",
      "The current board looks like this:\n",
      "[ 1  1  0 -1]\n",
      "\n",
      "*******\n",
      "You won!\n",
      "Play again? [y/n]y\n",
      "Please select difficulty [(easy/hard]h\n",
      "Would you like to start? [(y/n]n\n",
      "--------------------------------------------------------\n",
      "Welcome to a new game of connect 2 against AlphaGo Zero!\n",
      "--------------------------------------------------------\n",
      "\n",
      "The current board looks like this:\n",
      "[0 0 0 0]\n",
      "\n",
      "AlphaGo Zero's turn\n",
      "AlphaGo Zero belives it has a 70% chance of winning from here.\n",
      "\n",
      "This is what is looks like inside AlphaGo Zero's brain:\n",
      "[0.0406881  0.27292445 0.62337816 0.06300935]\n",
      "\n",
      "AlphaGo Zero's move is: 2\n",
      "------\n",
      "\n",
      "\n",
      "The current board looks like this:\n",
      "[ 0  0 -1  0]\n",
      "\n",
      "It is your turn.\n",
      "AlphaGo belives you have a 19% chance of winning from here.\n",
      "\n",
      "Pick a valid board position to move: [0 1 3]\n",
      "Your move is: 1\n",
      "------\n",
      "\n",
      "The current board looks like this:\n",
      "[ 0  1 -1  0]\n",
      "\n",
      "AlphaGo Zero's turn\n",
      "AlphaGo Zero belives it has a 62% chance of winning from here.\n",
      "\n",
      "This is what is looks like inside AlphaGo Zero's brain:\n",
      "[0.18341531 0.         0.         0.8165847 ]\n",
      "\n",
      "AlphaGo Zero's move is: 3\n",
      "------\n",
      "\n",
      "\n",
      "The current board looks like this:\n",
      "[ 0  1 -1 -1]\n",
      "\n",
      "*******\n",
      "Oh no, it's looks like you lost!\n",
      "Play again? [y/n]y\n",
      "Please select difficulty [(easy/hard]h\n",
      "Would you like to start? [(y/n]n\n",
      "--------------------------------------------------------\n",
      "Welcome to a new game of connect 2 against AlphaGo Zero!\n",
      "--------------------------------------------------------\n",
      "\n",
      "The current board looks like this:\n",
      "[0 0 0 0]\n",
      "\n",
      "AlphaGo Zero's turn\n",
      "AlphaGo Zero belives it has a 70% chance of winning from here.\n",
      "\n",
      "This is what is looks like inside AlphaGo Zero's brain:\n",
      "[0.0406881  0.27292445 0.62337816 0.06300935]\n",
      "\n",
      "AlphaGo Zero's move is: 2\n",
      "------\n",
      "\n",
      "\n",
      "The current board looks like this:\n",
      "[ 0  0 -1  0]\n",
      "\n",
      "It is your turn.\n",
      "AlphaGo belives you have a 19% chance of winning from here.\n",
      "\n",
      "Pick a valid board position to move: [0 1 3]\n",
      "Your move is: 3\n",
      "------\n",
      "\n",
      "The current board looks like this:\n",
      "[ 0  0 -1  1]\n",
      "\n",
      "AlphaGo Zero's turn\n",
      "AlphaGo Zero belives it has a 82% chance of winning from here.\n",
      "\n",
      "This is what is looks like inside AlphaGo Zero's brain:\n",
      "[0.02646521 0.97353476 0.         0.        ]\n",
      "\n",
      "AlphaGo Zero's move is: 1\n",
      "------\n",
      "\n",
      "\n",
      "The current board looks like this:\n",
      "[ 0 -1 -1  1]\n",
      "\n",
      "*******\n",
      "Oh no, it's looks like you lost!\n",
      "Play again? [y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    while True:\n",
    "        difficulty = input('Please select difficulty [(easy/hard]')\n",
    "        if difficulty not in ['easy', 'hard', 'e', 'h']:\n",
    "            print('Sorry, I do not understand what you just typed, please try again')\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    while True:\n",
    "        player_start = input('Would you like to start? [(y/n]')\n",
    "        if player_start not in ['y','n']:\n",
    "            print('Sorry, I do not understand what you just typed, please try again')\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    board = np.array([0,0,0,0])\n",
    "    print('--------------------------------------------------------')\n",
    "    print('Welcome to a new game of connect 2 against AlphaGo Zero!')\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    player_start = True if player_start == 'y' else False\n",
    "    while True:\n",
    "        if player_start in [True, None]:\n",
    "            player_start = None\n",
    "            print(f'\\nThe current board looks like this:\\n{board}\\n')\n",
    "            valid_moves = get_valid_moves(board)\n",
    "            reward = end_game(board, valid_moves)\n",
    "            if reward == -1:\n",
    "                print('*******')\n",
    "                print(\"Oh no, it's looks like you lost!\")\n",
    "                break\n",
    "            elif reward == 0:\n",
    "                print('*******')\n",
    "                print(\"It's a draw!\")\n",
    "                break\n",
    "\n",
    "            print('It is your turn.')\n",
    "            chance_of_win = alpha_go.value_model.predict(np.expand_dims(board, axis=0)).flatten()[0]\n",
    "            print(f'AlphaGo belives you have a {(chance_of_win+1)/2:.0%} chance of winning from here.\\n')\n",
    "            while True:\n",
    "                print(f'Pick a valid board position to move: {valid_moves}')\n",
    "                action = int(input('Your move is: '))\n",
    "                if action not in valid_moves:\n",
    "                    print('\\nYour action is invalid, please try again')\n",
    "                else:\n",
    "                    break\n",
    "            board[action] = 1\n",
    "            print('------')\n",
    "\n",
    "        if player_start in [False, None]:\n",
    "            player_start = None\n",
    "            print(f'\\nThe current board looks like this:\\n{board}\\n')\n",
    "            board = flip_board(board)\n",
    "            valid_moves = get_valid_moves(board)\n",
    "            reward = end_game(board, valid_moves)\n",
    "            if reward == -1:\n",
    "                print('*******')\n",
    "                print(\"You won!\")\n",
    "                break\n",
    "            elif reward == 0:\n",
    "                print('*******')\n",
    "                print(\"It's a draw!\")\n",
    "                break\n",
    "\n",
    "            print(\"AlphaGo Zero's turn\")\n",
    "            chance_of_win = alpha_go.value_model.predict(np.expand_dims(board, axis=0)).flatten()[0]\n",
    "            print(f'AlphaGo Zero belives it has a {(chance_of_win+1)/2:.0%} chance of winning from here.\\n')\n",
    "            alpha_actions = alpha_go.policy_model.predict(np.expand_dims(board, axis=0)).flatten()\n",
    "            alpha_actions = clean_nn_probs(board, alpha_actions)\n",
    "            print(f\"This is what is looks like inside AlphaGo Zero's brain:\\n{alpha_actions}\\n\")\n",
    "            if difficulty == 'easy':\n",
    "                alpha_action = np.random.choice([0,1,2,3], p=rescale_probs)\n",
    "            else:\n",
    "                alpha_action = np.argmax(alpha_actions)\n",
    "            print(f\"AlphaGo Zero's move is: {alpha_action}\")\n",
    "            board[alpha_action] = 1\n",
    "            board = flip_board(board)\n",
    "            print('------\\n')\n",
    "\n",
    "    play_again = input('Play again? [y/n]')\n",
    "    if play_again != 'y':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
