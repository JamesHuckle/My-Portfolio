{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yup\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Jameshuckle\\Dropbox\\My-Portfolio\\AlgorithmicTrading\\utils')\n",
    "from trading_util import (download_data_local_check, prep_stock_data, prep_fx_data, calc_sharpe, calc_romad)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mplfinance as mpf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print('yup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURUSD_1h_2003-2010.csv\n",
      "EURUSD_1h_2010-2020.csv\n",
      "USDJPY_1h_2003-2010.csv\n",
      "USDJPY_1h_2010-2020.csv\n",
      "NZDUSD_1h_2003-2020.csv\n",
      "AUDUSD_1h_2003-2020.csv\n",
      "USDCAD_1h_2003-2020.csv\n"
     ]
    }
   ],
   "source": [
    "data_source = 'fx' # 'fx', 'stock'\n",
    "\n",
    "if data_source == 'fx':\n",
    "    ### FX data #######\n",
    "    fx_files = [\n",
    "                 'EURUSD_1h_2003-2020.csv',\n",
    "                 'USDJPY_1h_2003-2020.csv',\n",
    "                 'NZDUSD_1h_2003-2020.csv',\n",
    "                 'AUDUSD_1h_2003-2020.csv',\n",
    "                 'USDCAD_1h_2003-2020.csv',\n",
    "                 ]\n",
    "\n",
    "    loaded_files = prep_fx_data(fx_files)\n",
    "        \n",
    "if data_source == 'stock':\n",
    "    ### stock data ######\n",
    "    start = '2000-01-01'\n",
    "    end = '2020-04-28'\n",
    "    ## download data\n",
    "    all_stock_data = download_data_local_check('SP500', start, end)\n",
    "    loaded_files = prep_stock_data(all_stock_data, filter_start_date_tuple=None) #(2015,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove candlestick embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.12284</td>\n",
       "      <td>1.12338</td>\n",
       "      <td>1.12160</td>\n",
       "      <td>1.12169</td>\n",
       "      <td>4263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.12161</td>\n",
       "      <td>1.13009</td>\n",
       "      <td>1.12014</td>\n",
       "      <td>1.12924</td>\n",
       "      <td>4667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.12921</td>\n",
       "      <td>1.14506</td>\n",
       "      <td>1.12723</td>\n",
       "      <td>1.14234</td>\n",
       "      <td>4858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.14218</td>\n",
       "      <td>1.14323</td>\n",
       "      <td>1.13265</td>\n",
       "      <td>1.13494</td>\n",
       "      <td>3211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.13507</td>\n",
       "      <td>1.15077</td>\n",
       "      <td>1.13006</td>\n",
       "      <td>1.14820</td>\n",
       "      <td>5848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High      Low    Close label\n",
       "0  1.12284  1.12338  1.12160  1.12169  4263\n",
       "1  1.12161  1.13009  1.12014  1.12924  4667\n",
       "2  1.12921  1.14506  1.12723  1.14234  4858\n",
       "3  1.14218  1.14323  1.13265  1.13494  3211\n",
       "4  1.13507  1.15077  1.13006  1.14820  5848"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def open_diff(numpy_ohlc):\n",
    "    diff_data_numpy = np.zeros(numpy_ohlc.shape)\n",
    "    # open vs close\n",
    "    diff_data_numpy[1:,0] = (numpy_ohlc[1:,0] - numpy_ohlc[:-1,3]) / numpy_ohlc[:-1,3]\n",
    "    # high, low, close vs open\n",
    "    diff_data_numpy[:,[1,2,3]] = ((numpy_ohlc[:,[1,2,3]].T - numpy_ohlc[:,0]) / numpy_ohlc[:,0]).T \n",
    "    return diff_data_numpy\n",
    "\n",
    "def remove_dates(raw_data):\n",
    "    dates = raw_data.index\n",
    "    raw_data = raw_data.reset_index(drop=True)\n",
    "    return raw_data, dates\n",
    "\n",
    "def diff(raw_data):\n",
    "    diff_data = open_diff(raw_data)\n",
    "    diff_data = pd.DataFrame(diff_data, columns=['open_diff','high_diff','low_diff','close_diff'])\n",
    "    return diff_data\n",
    "\n",
    "def scale(diff_data, train=True):\n",
    "    if train:\n",
    "        global data_scaler\n",
    "        data_scaler = StandardScaler()\n",
    "        scale_data = data_scaler.fit_transform(diff_data)\n",
    "    else:\n",
    "        scale_data = data_scaler.transform(diff_data)\n",
    "    scale_data = pd.DataFrame(scale_data, columns=['open_scale','high_scale','low_scale','close_scale'])\n",
    "    return scale_data\n",
    "\n",
    "def scale_bins(scale_data, num_bins=5):\n",
    "    cols = ['open_scale','high_scale','low_scale','close_scale']\n",
    "    for col in cols:\n",
    "        scale_data[f'{col}_bins'] = pd.cut(scale_data[col], num_bins, labels=False)\n",
    "    for col in cols:\n",
    "        scale_data[f'{col}_bins_label'] = pd.cut(scale_data[col], num_bins)\n",
    "        \n",
    "    bin_cols = [f'{col}_bins' for col in cols]\n",
    "    scale_data[bin_cols] = scale_data[bin_cols].astype(int).astype(str)\n",
    "    scale_data['label'] = scale_data[bin_cols].agg(''.join, axis=1)\n",
    "    return scale_data\n",
    "\n",
    "def create_candlestick_corpus(raw_data, train=True, pandas_with_dates=True):\n",
    "    if pandas_with_dates:\n",
    "        raw_data, dates = remove_dates(raw_data)\n",
    "        diff_data = diff(raw_data.to_numpy())\n",
    "    else:\n",
    "        diff_data = diff(raw_data)\n",
    "        raw_data = pd.DataFrame(raw_data)\n",
    "        raw_data.columns = ['Open','High','Low','Close']\n",
    "    scale_data = scale(diff_data, train=train)\n",
    "    scale_data_bins = scale_bins(scale_data, num_bins=[-np.inf, -1.5, -1, -0.6, -0.1, 0.1, 0.6, 1, 1.5, np.inf])\n",
    "    data = pd.concat([raw_data, scale_data_bins['label']], axis=1)\n",
    "    if pandas_with_dates:\n",
    "        data.index = dates\n",
    "    else:\n",
    "        data = data.to_numpy()\n",
    "    return data\n",
    "\n",
    "def create_candlestick_corpus_all():\n",
    "    all_raw_data = []\n",
    "    all_scale_data = []\n",
    "    for file, data in loaded_files.items():\n",
    "        raw_data = data[['Open','High','Low','Close']]\n",
    "        raw_data = raw_data.resample('1D').agg({'Open':'first','High':'max','Low':'min','Close':'last'})\n",
    "        raw_data.dropna(inplace=True)    \n",
    "        raw_data, dates = remove_dates(raw_data)\n",
    "        diff_data = diff(raw_data.to_numpy())\n",
    "        scale_data = scale(diff_data, train=True)\n",
    "        all_raw_data.append(raw_data) \n",
    "        all_scale_data.append(scale_data) \n",
    "    raw_data = pd.concat(all_raw_data, axis=0)\n",
    "    scale_data = pd.concat(all_scale_data, axis=0)\n",
    "    scale_data_bins = scale_bins(scale_data, num_bins=[-np.inf, -1.5, -1, -0.6, -0.1, 0.1, 0.6, 1, 1.5, np.inf])\n",
    "    data = pd.concat([raw_data, scale_data_bins['label']], axis=1)\n",
    "    return data\n",
    "\n",
    "def plot_candlestick_types(candle_one_str, candle_two_str, num_candles):\n",
    "    s  = mpf.make_mpf_style(base_mpf_style='yahoo', rc={'font.size':20})\n",
    "    candle_one_filter = all_data.query('label == @candle_one_str').head(num_candles)\n",
    "    candle_two_filter = all_data.query('label == @candle_two_str').head(num_candles)\n",
    "    print('there are',len(candle_one_filter), 'candle_one')\n",
    "    print('there are',len(candle_two_filter), 'candle_two')\n",
    "    filtered = pd.concat([candle_one_filter, candle_two_filter], axis= 0)[['open_diff','high_diff','low_diff','close_diff']]\n",
    "    filtered.columns = ['Open','High','Low','Close']\n",
    "    filtered[['High','Low','Close']] = (filtered[['High','Low','Close']].T + filtered['Open']).T\n",
    "    mpf.plot(filtered, type='candle', figscale=2)\n",
    "\n",
    "# #all_data = all_data_steps(raw_data, train=True)\n",
    "# #plot_candlestick_types(candle_one_str='4444', candle_two_str='8868', num_candles=10)\n",
    "# corpus_data = create_candlestick_corpus(raw_data, train=True, pandas_with_dates=True)\n",
    "\n",
    "corpus_data = create_candlestick_corpus_all()\n",
    "corpus_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25858, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_data['label'].to_numpy()\n",
    "words_to_index = {word:idx for idx, word in enumerate(set(corpus))}\n",
    "corpus_ids = [words_to_index[word] for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def asymetric_window_co_occurrence_matrix(window=10):\n",
    "    co_occur = {}\n",
    "    for idx in range(len(corpus_ids) - window):\n",
    "        candles_window = corpus_ids[idx: idx + window]\n",
    "        target_word = candles_window[-1]\n",
    "        co_occur.setdefault(target_word, {})\n",
    "        for score, candle in enumerate(candles_window[:-1]):\n",
    "            co_occur.setdefault(candle, {}).setdefault(target_word, 0)\n",
    "            co_occur[target_word].setdefault(candle, 0) \n",
    "            co_occur[target_word][candle] += ((score + 1))/10   \n",
    "            co_occur[candle][target_word] += ((score + 1))/10\n",
    "    return co_occur\n",
    "\n",
    "co_occur = asymetric_window_co_occurrence_matrix(window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_occur_matrix = pd.DataFrame(co_occur)\n",
    "# co_occur_matrix = co_occur_matrix.loc[co_occur_matrix.columns]\n",
    "# co_occur_matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_model(vocab_size, vector_size):\n",
    "    \n",
    "    w_i = layers.Input(shape=(1,))\n",
    "    w_j = layers.Input(shape=(1,))\n",
    "\n",
    "    emb_i = layers.Flatten()(layers.Embedding(vocab_size, vector_size, input_length=1)(w_i))\n",
    "    emb_j = layers.Flatten()(layers.Embedding(vocab_size, vector_size, input_length=1)(w_j))\n",
    "\n",
    "    ij_dot = layers.Dot(axes=-1)([emb_i, emb_j])\n",
    "    \n",
    "    b_i = layers.Flatten()(layers.Embedding(vocab_size, 1, input_length=1)(w_i))\n",
    "    b_j = layers.Flatten()(layers.Embedding(vocab_size, 1, input_length=1)(w_j))\n",
    "\n",
    "    pred = layers.Add()([ij_dot, b_i, b_j])\n",
    "\n",
    "    model = Model(inputs=[w_i, w_j], outputs=pred)\n",
    "    return model\n",
    "    \n",
    "def glove_loss(y_true, y_pred):\n",
    "    alpha = 0.75\n",
    "    x_max = 100\n",
    "    f_x = K.pow(K.maximum(y_true, x_max) / x_max, alpha)\n",
    "    loss = f_x * K.square(y_pred - K.log(y_true))\n",
    "    return K.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(co_occur):\n",
    "    first, second, x_ijs = [], [], []\n",
    "\n",
    "    for first_id in co_occur.keys():\n",
    "        for second_id in co_occur[first_id].keys():\n",
    "            x_ij = co_occur[first_id][second_id]\n",
    "\n",
    "            # add (main, context) pair\n",
    "            first.append(first_id)\n",
    "            second.append(second_id)\n",
    "            x_ijs.append(x_ij)\n",
    "\n",
    "            # add (context, main) pair\n",
    "            first.append(second_id)\n",
    "            second.append(first_id)\n",
    "            x_ijs.append(x_ij)\n",
    "\n",
    "    return np.array(first), np.array(second), np.array(x_ijs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_indices, second_indices, frequencies = create_input(co_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 4\n",
    "model = create_glove_model(vocab_size=len(co_occur.keys()), vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "model.compile(loss=glove_loss, optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 214770 samples\n",
      "Epoch 1/20\n",
      "214770/214770 [==============================] - 1s 5us/sample - loss: 446.1743\n",
      "Epoch 2/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.2142\n",
      "Epoch 3/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1880\n",
      "Epoch 4/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.2148\n",
      "Epoch 5/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1692\n",
      "Epoch 6/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1887\n",
      "Epoch 7/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1806\n",
      "Epoch 8/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.2188\n",
      "Epoch 9/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1832\n",
      "Epoch 10/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.2036\n",
      "Epoch 11/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1920\n",
      "Epoch 12/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1944\n",
      "Epoch 13/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.2096\n",
      "Epoch 14/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1939\n",
      "Epoch 15/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.2050\n",
      "Epoch 16/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1932\n",
      "Epoch 17/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.2197\n",
      "Epoch 18/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1813\n",
      "Epoch 19/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1915\n",
      "Epoch 20/20\n",
      "214770/214770 [==============================] - 1s 3us/sample - loss: 446.1984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20ea33d0d08>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([first_indices, second_indices], frequencies, epochs=20, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = model.layers[2].get_weights()[0] + model.layers[3].get_weights()[0]\n",
    "candlestick_embeddings = dict(zip(list(words_to_index.keys()), embeddings))\n",
    "#candlestick_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'candlestick_embeddings_{vector_size}.pkl','wb') as f:\n",
    "    pickle.dump(candlestick_embeddings, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
