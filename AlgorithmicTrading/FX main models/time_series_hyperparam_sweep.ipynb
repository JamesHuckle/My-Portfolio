{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time_series_deep_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name': 'keras-1',\n",
    "    'program': 'time_series_hyperparam_sweep.py',\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        'units': {'values': [5, 10, 20, 50, 100, 200]},\n",
    "        'layers': {'values': [3, 5, 7, 10, 20, 40, 60]},\n",
    "        'epochs': {'values': [150]},\n",
    "        'window': {'values': [5, 7, 10, 15, 20, 30, 50, 100]},\n",
    "        'num_bars': {'values': [5, 7, 10, 15, 20, 30, 50, 100]},\n",
    "        'norm_by_vol': {'values': [False, False, False, False, True]},\n",
    "        'resample': {'values': ['1D']},\n",
    "        'close_only': {'values': [False, False, False, False, True]},\n",
    "        'target_stop': {'values': [False, False, False, False, True]},\n",
    "        'model_arch': {'values': ['lstm','lstm','dnn']},\n",
    "        'l1_reg': {'values': [0, 0, 0, 0, 1e-8, 1e-7, 1e-6]},\n",
    "        'l2_reg': {'values': [0, 0, 0, 0, 0, 1e-7, 1e-6, 1e-5]},\n",
    "        'drop_rate': {'values': [0, 0, 0, 0, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4]},\n",
    "        'lr': {'values': [1e-3, 1e-4, 1e-5]},\n",
    "        'problem_type': {'values': ['binary', 'binary', 'binary', 'category', 'regression']},\n",
    "        'standardize': {'values': ['min_max', 'min_max','std', 'std', False]},\n",
    "        'pca_features': {'values': [True, False, False, False, False]},\n",
    "####  \n",
    "#         'epochs': {'values': [150]},\n",
    "#         'close_only': {'values': [False]},       \n",
    "#         'drop_rate': {'values': [0.2]},\n",
    "#         'l1_reg': {'values': [1e-8]},\n",
    "#         'l2_reg': {'values': [0]},\n",
    "#         'layers': {'values': [3]},\n",
    "#         'lr': {'values': [1e-3]},\n",
    "#         'model_arch': {'values': ['lstm']}, #set lstm_layers size in code if 'lstm'\n",
    "#         'norm_by_vol': {'values': [False]},\n",
    "#         'num_bars': {'values': [100]},  \n",
    "#         'pca_features': {'values': [False]}, #set pca_fraction size in code if TRUE\n",
    "#         'problem_type': {'values': ['category']}, #set std_thresh size in code if 'category'\n",
    "#         'resample': {'values': ['1H']},\n",
    "#         'standardize': {'values': ['min_max']},\n",
    "#         'target_stop': {'values': [False]},  #set stop_target size in code if True\n",
    "#         'units': {'values': [10]},\n",
    "#         'window': {'values': [5]},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: mqlee0wr\n",
      "Sweep URL: https://wandb.ai/jameshuckle/timeseries-4/sweeps/mqlee0wr\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='timeseries-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURUSD_1h_2003-2020.csv\n",
      "GBPUSD_1h_2003-2020.csv\n",
      "USDJPY_1h_2003-2020.csv\n",
      "NZDUSD_1h_2003-2020.csv\n",
      "AUDUSD_1h_2003-2020.csv\n",
      "USDCAD_1h_2003-2020.csv\n",
      "USDCHF_1h_2003-2020.csv\n"
     ]
    }
   ],
   "source": [
    "data_source = 'stock' # 'fx', 'stock'\n",
    "\n",
    "if data_source == 'fx':\n",
    "    ### FX data #######\n",
    "    fx_files = [\n",
    "                 'EURUSD_1h_2003-2020.csv',\n",
    "                 'GBPUSD_1h_2003-2020.csv',\n",
    "                 'USDJPY_1h_2003-2020.csv',\n",
    "                 'NZDUSD_1h_2003-2020.csv',\n",
    "                 'AUDUSD_1h_2003-2020.csv',\n",
    "                 'USDCAD_1h_2003-2020.csv',\n",
    "                 'USDCHF_1h_2003-2020.csv',\n",
    "                 ]\n",
    "\n",
    "    loaded_files = prep_fx_data(fx_files)\n",
    "        \n",
    "if data_source == 'stock':\n",
    "    ### stock data ######\n",
    "    start = '2000-01-01'\n",
    "    end = '2020-04-28'\n",
    "    ## download data\n",
    "    all_stock_data = download_data_local_check('SP500', start, end)\n",
    "    loaded_files = prep_stock_data(all_stock_data, filter_start_date_tuple=None) #(2015,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# this config is only used to test the code in the notebook\n",
    "test_config = {\n",
    "    'units': 100,\n",
    "    'layers': 10,\n",
    "    'epochs': 10,\n",
    "    'window': 50,\n",
    "    'num_bars': 50,\n",
    "    'norm_by_vol': False,\n",
    "    'resample': '1D',\n",
    "    'close_only': True,\n",
    "    'target_stop': True,\n",
    "    'model_arch': 'lstm',\n",
    "    'l1_reg': 1e-7,\n",
    "    'l2_reg': 1e-7,\n",
    "    'drop_rate': 0.1,\n",
    "    'lr': 1e-35,\n",
    "    'problem_type': 'binary',\n",
    "    'standardize': 'min_max',\n",
    "    'pca_features': False, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 64\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 75\n",
      "final_dataset len(x) 4777 len(y) 4777\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 38\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 42\n",
      "final_dataset len(x) 4777 len(y) 4777\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 10\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 113\n",
      "final_dataset len(x) 4777 len(y) 4777\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 5 # hit_neither_target_or_stop: 110\n",
      "final_dataset len(x) 4777 len(y) 4777\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 1 # hit_neither_target_or_stop: 75\n",
      "final_dataset len(x) 4777 len(y) 4777\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 17\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 67\n",
      "final_dataset len(x) 4777 len(y) 4777\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 46\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 66\n",
      "final_dataset len(x) 4777 len(y) 4777\n",
      "train_validation: 0.8\n",
      "caching: C:\\Users\\Jameshuckle\\Documents\\Algo_Trading\\data\\all_data_train_cache\n",
      "caching: C:\\Users\\Jameshuckle\\Documents\\Algo_Trading\\data\\all_data_test_cache\n",
      "Epoch 1/10\n",
      "54/54 - 2s - loss: 0.7081 - accuracy: 0.5247 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 2/10\n",
      "54/54 - 1s - loss: 0.7083 - accuracy: 0.5250 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 3/10\n",
      "54/54 - 1s - loss: 0.7075 - accuracy: 0.5247 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 4/10\n",
      "54/54 - 1s - loss: 0.7076 - accuracy: 0.5247 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 5/10\n",
      "54/54 - 1s - loss: 0.7074 - accuracy: 0.5255 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 6/10\n",
      "54/54 - 1s - loss: 0.7072 - accuracy: 0.5254 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 7/10\n",
      "54/54 - 1s - loss: 0.7083 - accuracy: 0.5257 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 8/10\n",
      "54/54 - 1s - loss: 0.7073 - accuracy: 0.5249 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 9/10\n",
      "54/54 - 1s - loss: 0.7080 - accuracy: 0.5244 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "Epoch 10/10\n",
      "54/54 - 1s - loss: 0.7069 - accuracy: 0.5249 - val_loss: 0.7259 - val_accuracy: 0.4834\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Jameshuckle\\Documents\\Algo_Trading\\model_checkpoints\\model_checkpoints\\model.ckpt\\assets\n",
      "\n",
      "---------------\n",
      "\n",
      "total_epochs 10\n",
      "Loaded weights from best val_accuracy\n",
      "best val_accuracy: 0.4834 | epoch=0\n",
      "EURUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 64\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 75\n",
      "avg_profit (1 pip fees) 0.002879953126721959\n",
      "best possible profit (1 pip fees) 0.01805356717713569\n",
      "no. trades: 517\n",
      "GBPUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 38\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 42\n",
      "avg_profit (1 pip fees) 0.0030669901091483205\n",
      "best possible profit (1 pip fees) 0.02889525125053003\n",
      "no. trades: 517\n",
      "USDJPY_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 10\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 113\n",
      "avg_profit (1 pip fees) -0.005784968357580043\n",
      "best possible profit (1 pip fees) 0.019138776552777775\n",
      "no. trades: 517\n",
      "NZDUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 5 # hit_neither_target_or_stop: 110\n",
      "avg_profit (1 pip fees) 0.006609168293783911\n",
      "best possible profit (1 pip fees) 0.026628678931492948\n",
      "no. trades: 517\n",
      "AUDUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 1 # hit_neither_target_or_stop: 75\n",
      "avg_profit (1 pip fees) 0.004873061444711492\n",
      "best possible profit (1 pip fees) 0.0281049547296014\n",
      "no. trades: 517\n",
      "USDCAD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 17\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 67\n",
      "avg_profit (1 pip fees) -0.010688215417449772\n",
      "best possible profit (1 pip fees) 0.018871440647407273\n",
      "no. trades: 517\n",
      "USDCHF_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 46\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 66\n",
      "avg_profit (1 pip fees) -0.011727072934820506\n",
      "best possible profit (1 pip fees) 0.019538446265731937\n",
      "no. trades: 517\n",
      "suspect high stock trade: {}\n",
      "suspect low stock trades: {}\n",
      "averge profit (after 1 pip fees): -0.001538726247926377\n",
      "Loaded weights from best val_loss\n",
      "best val_loss: 0.7259 | epoch=0\n",
      "EURUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 64\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 75\n",
      "avg_profit (1 pip fees) 0.002879953126721959\n",
      "best possible profit (1 pip fees) 0.01805356717713569\n",
      "no. trades: 517\n",
      "GBPUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 38\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 42\n",
      "avg_profit (1 pip fees) 0.0030669901091483205\n",
      "best possible profit (1 pip fees) 0.02889525125053003\n",
      "no. trades: 517\n",
      "USDJPY_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 10\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 113\n",
      "avg_profit (1 pip fees) -0.005784968357580043\n",
      "best possible profit (1 pip fees) 0.019138776552777775\n",
      "no. trades: 517\n",
      "NZDUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 5 # hit_neither_target_or_stop: 110\n",
      "avg_profit (1 pip fees) 0.006609168293783911\n",
      "best possible profit (1 pip fees) 0.026628678931492948\n",
      "no. trades: 517\n",
      "AUDUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 1 # hit_neither_target_or_stop: 75\n",
      "avg_profit (1 pip fees) 0.004873061444711492\n",
      "best possible profit (1 pip fees) 0.0281049547296014\n",
      "no. trades: 517\n",
      "USDCAD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 17\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 67\n",
      "avg_profit (1 pip fees) -0.010688215417449772\n",
      "best possible profit (1 pip fees) 0.018871440647407273\n",
      "no. trades: 517\n",
      "USDCHF_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 46\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 66\n",
      "avg_profit (1 pip fees) -0.011727072934820506\n",
      "best possible profit (1 pip fees) 0.019538446265731937\n",
      "no. trades: 517\n",
      "suspect high stock trade: {}\n",
      "suspect low stock trades: {}\n",
      "averge profit (after 1 pip fees): -0.001538726247926377\n",
      "load weights from epoch 10\n",
      "EURUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 64\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 75\n",
      "avg_profit (1 pip fees) 0.002879953126721959\n",
      "best possible profit (1 pip fees) 0.01805356717713569\n",
      "no. trades: 517\n",
      "GBPUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 38\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 42\n",
      "avg_profit (1 pip fees) 0.0030669901091483205\n",
      "best possible profit (1 pip fees) 0.02889525125053003\n",
      "no. trades: 517\n",
      "USDJPY_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 10\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 113\n",
      "avg_profit (1 pip fees) -0.005784968357580043\n",
      "best possible profit (1 pip fees) 0.019138776552777775\n",
      "no. trades: 517\n",
      "NZDUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 5 # hit_neither_target_or_stop: 110\n",
      "avg_profit (1 pip fees) 0.006609168293783911\n",
      "best possible profit (1 pip fees) 0.026628678931492948\n",
      "no. trades: 517\n",
      "AUDUSD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 39\n",
      "# hit_target_and_stop: 1 # hit_neither_target_or_stop: 75\n",
      "avg_profit (1 pip fees) 0.004873061444711492\n",
      "best possible profit (1 pip fees) 0.0281049547296014\n",
      "no. trades: 517\n",
      "USDCAD_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 17\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 67\n",
      "avg_profit (1 pip fees) -0.010688215417449772\n",
      "best possible profit (1 pip fees) 0.018871440647407273\n",
      "no. trades: 517\n",
      "USDCHF_1h_2003-2020.csv\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 46\n",
      "# hit_target_and_stop: 0 # hit_neither_target_or_stop: 66\n",
      "avg_profit (1 pip fees) -0.011727072934820506\n",
      "best possible profit (1 pip fees) 0.019538446265731937\n",
      "no. trades: 517\n",
      "suspect high stock trade: {}\n",
      "suspect low stock trades: {}\n",
      "averge profit (after 1 pip fees): -0.001538726247926377\n",
      "all_history dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "{'max_accuracy': 0.5256999731063843, 'max_val_accuracy': 0.48340311646461487, 'min_val_loss': 0.7259292006492615, 'min_loss': 0.7069039940834045, 'ROMAD_best_metric': -0.22, 'ROMAD_best_loss': -0.22, 'ROMAD_last': -0.22, 'ROMAD_raw': -0.21}\n"
     ]
    }
   ],
   "source": [
    "def train(test=False):\n",
    "    if test:\n",
    "        wb = test_config\n",
    "    else:\n",
    "        config = {k:v['values'][0] for k,v in sweep_config['parameters'].items()}\n",
    "        wandb.init(config=config, magic=False)\n",
    "        wb = wandb.config\n",
    "    sweep = True # works with my code to make modifications for a sweep\n",
    "    \n",
    "    class algo_variables():\n",
    "        pass\n",
    "\n",
    "    var = algo_variables()\n",
    "    var.window = wb['window'] # number of training bars\n",
    "    var.pca_features = wb['pca_features'] # False, 10\n",
    "    if var.pca_features:\n",
    "        fraction_of_features = np.random.choice([0.25, 0.5])\n",
    "        var.pca_features = int(wb['window'] * fraction_of_features)\n",
    "        wb.update({'pca_features':var.pca_features, 'pct_fraction':fraction_of_features},\n",
    "                  allow_val_change=True)\n",
    "    var.standardize = wb['standardize'] #'std', 'min_max'\n",
    "    var.norm_by_vol = wb['norm_by_vol'] #True\n",
    "    var.data_percentage_diff = 'close_diff' # False, 'close_diff', 'ohlc_diff', 'open_diff'\n",
    "    var.data_percentage_diff_y = True\n",
    "    var.train_split = datetime(2019,1,1) #0.9, datetime(2018,1,1)\n",
    "    var.resample = wb['resample'] # None '1D', '4H', '1W'\n",
    "    var.read_single_file = None #all_files[3] #None\n",
    "    var.loaded_files = loaded_files\n",
    "\n",
    "    var.num_bars = wb['num_bars'] # prediction horizon\n",
    "    var.problem_type = wb['problem_type'] #'regression' 'binary' 'category'  \n",
    "    if var.problem_type == 'category':\n",
    "        var.std_thresh = 1 # to determine a positive, negative or flat trade\n",
    "        if sweep:\n",
    "            var.std_thresh = 0.25 #np.random.choice([0.25,0.5])\n",
    "            wb.update({'std_thresh':var.std_thresh}, allow_val_change=True)\n",
    "    var.dataset_type = 'stock' #'wave', 'random', 'stock', 'monte_carlo'\n",
    "    var.close_only = wb['close_only']\n",
    "    if var.close_only:\n",
    "        var.cols = ['Close'] if var.dataset_type in ['stock','monte_carlo'] else ['univariate']\n",
    "    else:\n",
    "        var.cols = ['Open', 'High', 'Low', 'Close'] if var.dataset_type == 'stock' else ['univariate']\n",
    "    var.multi_y = False\n",
    "\n",
    "    var.input_len = var.pca_features if var.pca_features else var.window\n",
    "\n",
    "    ## target/stop binary outcomes (1 R/R) ##\n",
    "    var.target_stop = wb['target_stop']\n",
    "    if var.target_stop:\n",
    "        var.num_bars = 1 # must be equal to 1!\n",
    "        var.problem_type = 'binary'\n",
    "        var.dataset_type = 'stock'\n",
    "        var.close_only = False\n",
    "        var.cols = ['Open', 'High', 'Low', 'Close']\n",
    "        var.bar_horizon = 10000 # how long to wait for stop or target hit, otherwise assign current profit\n",
    "        var.bar_size_ma = 100 # how long is moving average for bar size (used to calc stop and target)\n",
    "        var.stop_target_size = 3 # size of stop and target relative to averge bar size\n",
    "        if sweep:\n",
    "            var.stop_target_size = np.random.choice([2,3,4])\n",
    "            wb.update({'stop_target_size':var.stop_target_size}, allow_val_change=True)\n",
    "\n",
    "    var.embeddings = False\n",
    "    var.embedding_type = None #None 'light'\n",
    "    if var.embeddings:\n",
    "        var.standardize = False \n",
    "        var.pca_features = False\n",
    "        var.vector_size = 200 # 200, 4\n",
    "        if var.embedding_type == 'light':\n",
    "            var.vector_size = 1\n",
    "\n",
    "    generator = True\n",
    "    if generator: \n",
    "        ## save all stocks to csv and tfrecords, then load tfrecords as dataset\n",
    "        var.train_validation = 0.8 #False # Uses traning data to create test set (for validation)\n",
    "        var.batch_size = 500\n",
    "        base_path = f'C:/Users/Jameshuckle/Documents/Algo_Trading/data'\n",
    "        save_numpy_to_csv_all_files(base_path, var)\n",
    "        train_dataset = create_tfrecord_dataset(f'{base_path}/all_data_train', var)\n",
    "        test_dataset = create_tfrecord_dataset(f'{base_path}/all_data_test', var)\n",
    "    else:\n",
    "        ### load single stock into numpy\n",
    "        (x, y, x_test, y_test, y_pct_diff, y_test_pct_diff, train_data_raw,\n",
    "         test_data_raw) = create_dataset(file_name=list(loaded_files.keys())[1], var=var)\n",
    "        train_dataset, test_dataset = [], []\n",
    "        \n",
    "    tf.keras.backend.clear_session()\n",
    "    ###\n",
    "    var.model_arch = wb['model_arch'] # 'dnn','lstm','conv1d','incept1d'\n",
    "    var.l1_reg = wb['l1_reg'] #1e-6\n",
    "    var.l2_reg = wb['l2_reg'] #1e-5\n",
    "    var.drop_rate = wb['drop_rate'] #0.1 #0.2\n",
    "    ###\n",
    "        \n",
    "    if sweep:\n",
    "        var.layers = wb['layers']\n",
    "        var.units = wb['units']\n",
    "        if var.model_arch == 'lstm':\n",
    "            var.lstm_layers = 2 #np.random.randint(1,4)\n",
    "            wb.update({'lstm_layers':var.lstm_layers}, allow_val_change=True)\n",
    "\n",
    "    model = get_model_arch(var.model_arch, var, sweep)\n",
    "    (total_epochs, all_history, checkpoint_path_base,\n",
    "     checkpoint_path_model) = reset_model_checkpoint(\n",
    "        path='B:Algo_Trading/model_checkpoints')\n",
    "\n",
    "    ################\n",
    "    metric = 'root_mean_squared_error' if var.problem_type == 'regression' else 'accuracy'\n",
    "    \n",
    "    # Do extra epochs if model is learning well\n",
    "    while total_epochs <= 1500:\n",
    "        # load model to keep continuity of epochs. To create new model run cell above.\n",
    "        if os.path.exists(checkpoint_path_model):\n",
    "            print('loading model')\n",
    "            model = tf.keras.models.load_model(checkpoint_path_model)\n",
    "\n",
    "        plot_lr_rate = False\n",
    "        decrease_lr_rate = False\n",
    "        validation = True\n",
    "        var.epochs = wb['epochs']\n",
    "        var.lr = wb['lr']\n",
    "\n",
    "        model = compile_model(model=model, lr=var.lr, var=var)\n",
    "        gc.collect()\n",
    "\n",
    "        checkpoint_path_cb = checkpoint_path_base+'/model_epoch-{epoch}.ckpt'\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_cb,\n",
    "                                                         save_best_only=False,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         monitor='val_accuracy',\n",
    "                                                         mode='max', verbose=0)\n",
    "\n",
    "\n",
    "        kwargs = {'verbose':2, 'epochs':var.epochs, 'initial_epoch':total_epochs, 'shuffle':False}\n",
    "        if test:\n",
    "            kwargs['callbacks'] = [cp_callback]\n",
    "        else:\n",
    "            kwargs['callbacks'] = [cp_callback, WandbCallback()]\n",
    "        kwargs = set_model_hyperparams(var.epochs, kwargs, plot_lr_rate, decrease_lr_rate,\n",
    "                                       validation, test_dataset, generator)\n",
    "\n",
    "        if generator:\n",
    "            ### Parellelize loading\n",
    "            history = model.fit(x=train_dataset, **kwargs)\n",
    "        else:\n",
    "            batch_size = 100\n",
    "            history = model.fit(x, y, batch_size=batch_size, **kwargs)\n",
    "\n",
    "        model.save(checkpoint_path_model)\n",
    "        print('\\n---------------')\n",
    "        total_epochs = var.epochs\n",
    "        print('\\ntotal_epochs',total_epochs)\n",
    "        gc.collect()\n",
    "\n",
    "        for key, value in history.history.items():\n",
    "            all_history.setdefault(key, [])\n",
    "            all_history[key] += value\n",
    "        \n",
    "        # if the val_loss or val metric imporve throughout epoch, continue training\n",
    "        epoch_size = 150\n",
    "        one_third = total_epochs - int(epoch_size * (2/3))\n",
    "        two_third = total_epochs - int(epoch_size * (1/3))\n",
    "        half = total_epochs - int(epoch_size * (1/2))\n",
    "        \n",
    "        mid_epoch_loss = np.mean(all_history['val_loss'][one_third:two_third])\n",
    "        end_epoch_loss = np.mean(all_history['val_loss'][two_third:])\n",
    "        best_loss_end_of_epoch = np.argmin(all_history['val_loss']) > half\n",
    "        \n",
    "        mid_epoch_metric = np.mean(all_history[f'val_{metric}'][one_third:two_third])\n",
    "        end_epoch_metric = np.mean(all_history[f'val_{metric}'][two_third:])\n",
    "        if metric == 'accuracy':\n",
    "            best_metric_end_of_epoch = np.argmax(all_history[f'val_{metric}']) > half\n",
    "            metric_better = ((end_epoch_metric - 0.0001) > mid_epoch_metric)\n",
    "        else:\n",
    "            best_metric_end_of_epoch = np.argmin(all_history[f'val_{metric}']) > half\n",
    "            metric_better = ((end_epoch_metric + 0.0001) < mid_epoch_metric)\n",
    "        \n",
    "        if (\n",
    "            (((end_epoch_loss + 0.0001) < mid_epoch_loss) and best_loss_end_of_epoch)\n",
    "                                              or\n",
    "            (((end_epoch_metric - 0.0001) > mid_epoch_metric) and best_metric_end_of_epoch)\n",
    "        ):\n",
    "            print(f'continue training for another {epoch_size} epochs')\n",
    "            wb.update({'epochs':total_epochs + epoch_size}, allow_val_change=True)\n",
    "            del_unneeded_checkpoints(checkpoint_path_base, all_history, metric)\n",
    "        else:\n",
    "            break     \n",
    "    ##################\n",
    "       \n",
    "    all_results = {}\n",
    "    for (epoch_name, ep, man_val_metric) in [('best_epoch_metric', 0, False),\n",
    "                                             ('best_epoch_loss', 0, 'val_loss'),\n",
    "                                             ('last_epoch', var.epochs, False)]:      \n",
    "        man_epoch_idx = ep #Set to 0 or False to choose best accuracy, otherwise choose epoch to load\n",
    "\n",
    "        model = explore_epoch(metric, man_epoch_idx, man_val_metric, checkpoint_path_base,\n",
    "                              all_history, model)\n",
    "\n",
    "        ###\n",
    "        pip_fees = 1\n",
    "        review_set = 'test' #'test' 'train' ' all'\n",
    "\n",
    "        all_returns, all_raw = out_of_sample_results(loaded_files, pip_fees, review_set, model, var)\n",
    "\n",
    "        ###\n",
    "        all_returns_final = pd.concat(all_returns, axis=1)\n",
    "        all_returns_final, suspect_stocks = drop_outliers(all_returns_final)\n",
    "\n",
    "        print(f'averge profit (after {pip_fees} pip fees):',np.nanmean(all_returns_final))\n",
    "        all_returns_final['profit'] = all_returns_final.sum(axis=1)\n",
    "        all_returns_final['returns'] = all_returns_final['profit'].cumsum()\n",
    "        # all_returns_final['returns'].plot(title='all returns (time scaled)')\n",
    "        # plt.show()\n",
    "        # all_returns_final['returns'].reset_index(drop=True).plot(title='all returns (no time)')\n",
    "        # plt.show()\n",
    "        \n",
    "        daily_pct_change = all_returns_final['profit'].resample('1D').sum()\n",
    "        romad = calc_romad(daily_pct_change, filter_large_trades=False, yearly_agg=np.median,\n",
    "                           plot=False)\n",
    "        \n",
    "        equity_curve = all_returns_final['returns'].reset_index(drop=True).values.tolist()\n",
    "        all_results[epoch_name] = {\n",
    "            'romad':romad,\n",
    "            'returns':equity_curve\n",
    "        }    \n",
    "        \n",
    "    all_raw_final = pd.concat(all_raw, axis=1)\n",
    "    all_raw_final.drop(suspect_stocks, axis='columns', inplace=True) \n",
    "    raw_daily_pct_change = all_raw_final.sum(axis=1).resample('1D').sum()\n",
    "    romad_raw = calc_romad(raw_daily_pct_change, filter_large_trades=False, yearly_agg=np.median,\n",
    "                           plot=False)\n",
    "    raw_equity = raw_daily_pct_change.cumsum().reset_index(drop=True).values.tolist()\n",
    "        \n",
    "\n",
    "    if sweep:\n",
    "        b_e = all_results['best_epoch_metric']['returns']\n",
    "        l_e = all_results['last_epoch']['returns']\n",
    "        table1 = wandb.Table(data=zip(list(range(len(b_e))),b_e), columns = [\"x\", \"y\"])  \n",
    "        table2 = wandb.Table(data=zip(list(range(len(l_e))),l_e), columns = [\"x\", \"y\"])  \n",
    "        table3 = wandb.Table(data=zip(list(range(len(raw_equity))), raw_equity), columns = [\"x\", \"y\"])  \n",
    "        print('all_history', all_history.keys())\n",
    "        metrics = {\n",
    "            f'max_{metric}': max(all_history[metric]),\n",
    "            f'max_val_{metric}': max(all_history[f'val_{metric}']),\n",
    "            'min_val_loss': min(all_history['val_loss']),\n",
    "            'min_loss': min(all_history['loss']),\n",
    "            'ROMAD_best_metric': all_results['best_epoch_metric']['romad'],\n",
    "            'ROMAD_best_loss': all_results['best_epoch_loss']['romad'],\n",
    "            'ROMAD_last': all_results['last_epoch']['romad'],\n",
    "            'ROMAD_raw': romad_raw,\n",
    "        }\n",
    "        if test:\n",
    "            print(metrics)\n",
    "        else:\n",
    "            metrics.update({\n",
    "                'custom_plot_best_epoch': wandb.plot.line(table1, \"x\", \"y\",\n",
    "                                                          title=\"Profit plot best epoch\"),\n",
    "                'custom_plot_last_epoch': wandb.plot.line(table2, \"x\", \"y\",\n",
    "                                                          title=\"Profit plot last epoch\"),\n",
    "                'custom_plot_raw_benchmark': wandb.plot.line(table3, \"x\", \"y\",\n",
    "                                                             title=\"Profit plot raw benchmark\"),\n",
    "            })\n",
    "            wandb.log(metrics)\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    train(test=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('Jameshuckle': virtualenv)",
   "language": "python",
   "name": "python37464bitjameshucklevirtualenv1ec6556667214fdb836c8fa1e3a8bcb5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
