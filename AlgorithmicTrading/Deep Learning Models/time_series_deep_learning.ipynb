{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(f'{os.path.dirname(os.getcwd())}/utils')\n",
    "from data_util import (download_data_local_check, prep_stock_data, prep_fx_data, calc_sharpe,\n",
    "                       calc_romad, create_dataset, deep_learning_dataset, display_stop_target,\n",
    "                       create_candlestick_corpus)\n",
    "from candlestick_embeddings_util import create_candlestick_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.metrics import (mean_squared_error, accuracy_score)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def global_set_seed(seed_value):\n",
    "#     os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "#     random.seed(seed_value)\n",
    "#     np.random.seed(seed_value)\n",
    "#     tf.random.set_seed(seed_value)\n",
    "# global_set_seed(0)\n",
    "#np.random.seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURUSD_1h_2003-2020.csv\n",
      "GBPUSD_1h_2003-2020.csv\n",
      "USDJPY_1h_2003-2020.csv\n",
      "NZDUSD_1h_2003-2020.csv\n",
      "AUDUSD_1h_2003-2020.csv\n",
      "USDCAD_1h_2003-2020.csv\n",
      "USDCHF_1h_2003-2020.csv\n"
     ]
    }
   ],
   "source": [
    "data_source = 'fx' # 'fx', 'stock'\n",
    "\n",
    "if data_source == 'fx':\n",
    "    ### FX data #######\n",
    "    fx_files = [\n",
    "                 'EURUSD_1h_2003-2020.csv',\n",
    "                 'GBPUSD_1h_2003-2020.csv',\n",
    "                 'USDJPY_1h_2003-2020.csv',\n",
    "                 'NZDUSD_1h_2003-2020.csv',\n",
    "                 'AUDUSD_1h_2003-2020.csv',\n",
    "                 'USDCAD_1h_2003-2020.csv',\n",
    "                 'USDCHF_1h_2003-2020.csv',\n",
    "                 ]\n",
    "\n",
    "    loaded_files = prep_fx_data(fx_files)\n",
    "        \n",
    "if data_source == 'stock':\n",
    "    ### stock data ######\n",
    "    start = '2000-01-01'\n",
    "    end = '2020-11-1'\n",
    "    ## download data\n",
    "    all_stock_data = download_data_local_check('SP500', start, end)\n",
    "    loaded_files = prep_stock_data(all_stock_data, filter_start_date_tuple=None) #(2015,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_numpy_to_csv_all_files(base_path, var):\n",
    "    (x, y, x_test, y_test, y_pct_diff, y_test_pct_diff, train_data_raw, test_data_raw,\n",
    "     all_data) = deep_learning_dataset(var, train_validation=var.train_validation) \n",
    "    pd.concat([x,y], axis=1).to_csv(f'{base_path}/all_data_train.csv', mode='w', header=True,\n",
    "                                    index=False)\n",
    "    pd.concat([x_test, y_test], axis=1).to_csv(f'{base_path}/all_data_test.csv', mode='w',\n",
    "                                               header=True, index=False)\n",
    "    gc.collect()\n",
    "\n",
    "def write_csv_to_tfrecords(file_name_no_extension):\n",
    "    ### Read csv into pandas and save to tfrecords\n",
    "        csv = pd.read_csv(f'{file_name_no_extension}.csv').values\n",
    "        with tf.io.TFRecordWriter(f'{file_name_no_extension}.tfrecords') as writer:\n",
    "            for row in csv:\n",
    "                features, label = row[:-1], row[-1]\n",
    "                example = tf.train.Example()\n",
    "                example.features.feature[\"features\"].float_list.value.extend(features)\n",
    "                example.features.feature[\"label\"].float_list.value.append(label)\n",
    "                writer.write(example.SerializeToString())\n",
    "        \n",
    "def parse_tfrecord(serialized_example, var):\n",
    "    input_vector_size = var.vector_size if var.embeddings else len(var.cols)\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature(shape=(var.input_len, input_vector_size),\n",
    "                                          dtype=tf.float32),\n",
    "        'label': tf.io.FixedLenFeature(shape=(1), dtype=tf.float32),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    #example['features'] = tf.reshape(example['features'], shape=(var.input_len, len(var.cols)))\n",
    "    \n",
    "    return example['features'], example['label']\n",
    "\n",
    "def create_tfrecord_dataset(file_name_no_extension, var):\n",
    "    ### Write csv to tfrecords\n",
    "    write_csv_to_tfrecords(file_name_no_extension)\n",
    "    ### Read in single file\n",
    "    dataset = tf.data.TFRecordDataset(filenames = [f'{file_name_no_extension}.tfrecords'])\n",
    "#     for row in dataset.take(1):\n",
    "#         print(parse_tfrecord(row, var))\n",
    "    ### Parse files\n",
    "    dataset = dataset.map(lambda x: parse_tfrecord(x, var), num_parallel_calls=4)\n",
    "    # remove old cache files\n",
    "    file_start = os.path.split(file_name_no_extension)[-1]\n",
    "    dir_path = os.path.dirname(file_name_no_extension)\n",
    "    for file in os.listdir(dir_path):\n",
    "        if f'{file_start}_cache' in file:\n",
    "            os.remove(f'{dir_path}/{file}')\n",
    "    ### Cache parsed file onto disk\n",
    "    dataset = dataset.cache(filename=f'{file_name_no_extension}_cache')\n",
    "    print('caching:',f'{file_name_no_extension}_cache')\n",
    "    dataset = dataset.batch(batch_size=var.batch_size).prefetch( \n",
    "        buffer_size=tf.data.experimental.AUTOTUNE) #.shuffle(buffer_size=1024, seed=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(l1_reg, l2_reg, drop_rate, input_len, output_layer, layer_kwargs, var):\n",
    "    input_vector_size = var.vector_size if var.embeddings else len(var.cols)\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Reshape((input_len * input_vector_size,), input_shape=[input_len, input_vector_size]),\n",
    "        layers.Dense(units=1000, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dense(units=1000, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dense(units=1000, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dropout(drop_rate),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dropout(drop_rate),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dropout(drop_rate),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dropout(drop_rate),\n",
    "        layers.Dense(units=500, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        output_layer,\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def dnn_model_sweep(l1_reg, l2_reg, drop_rate, input_len, output_layer, layer_kwargs, var):\n",
    "    input_vector_size = var.vector_size if var.embeddings else len(var.cols)\n",
    "    first = layers.Reshape((input_len * input_vector_size,), input_shape=[input_len, input_vector_size])\n",
    "    hidden_layers = []\n",
    "    for _ in range(var.layers):\n",
    "        hidden_layers.append(layers.Dropout(drop_rate))\n",
    "        hidden_layers.append(layers.Dense(units=var.units, activation='relu',\n",
    "                                          kernel_regularizer=l1_l2(l1_reg, l2_reg), **layer_kwargs))\n",
    "    model = tf.keras.Sequential([first] + hidden_layers + [output_layer])\n",
    "    return model\n",
    "\n",
    "def conv1d_model(l1_reg, l2_reg, drop_rate, input_len, output_layer, layer_kwargs, var):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv1D(filters=10, kernel_size=4, input_shape=[input_len, len(var.cols)],\n",
    "                      **layer_kwargs),        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=10, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        output_layer,\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def inception1d_model(l1_reg, l2_reg, drop_rate, input_len, output_layer, layer_kwargs, var):\n",
    "    input_img = layers.Input(shape=(input_len, 1))\n",
    "\n",
    "    layer_1 = layers.Conv1D(10, 1, padding='same', activation='relu', **layer_kwargs)(input_img)\n",
    "    layer_1 = layers.Conv1D(10, 5, padding='same', activation='relu', **layer_kwargs)(layer_1)\n",
    "\n",
    "    layer_2 = layers.Conv1D(10, 1, padding='same', activation='relu', **layer_kwargs)(input_img)\n",
    "    layer_2 = layers.Conv1D(10, 10, padding='same', activation='relu', **layer_kwargs)(layer_2)\n",
    "\n",
    "    layer_3 = layers.MaxPooling1D(3, strides=1, padding='same')(input_img)\n",
    "    layer_3 = layers.Conv1D(10, 1, padding='same', activation='relu', **layer_kwargs)(layer_3)\n",
    "\n",
    "    mid_1 = layers.concatenate([layer_1, layer_2, layer_3], axis = 2)\n",
    "    flat_1 = layers.Flatten()(mid_1)\n",
    "\n",
    "    dense_1 = layers.Dense(128, activation='relu', **layer_kwargs)(flat_1)\n",
    "    dense_2 = layers.Dense(64, activation='relu', **layer_kwargs)(dense_1)\n",
    "    dense_3 = layers.Dense(32, activation='relu', **layer_kwargs)(dense_2)\n",
    "    output = output_layer(dense_3)\n",
    "    model = tf.keras.models.Model([input_img], output)\n",
    "    return model\n",
    "\n",
    "def lstm_model(l1_reg, l2_reg, drop_rate, input_len, output_layer, layer_kwargs, var):\n",
    "    input_vector_size = var.vector_size if var.embeddings else len(var.cols)\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.LSTM(units=10, return_sequences=True, input_shape=[None, input_vector_size],\n",
    "                    **layer_kwargs),   \n",
    "        layers.LSTM(units=10, return_sequences=False, **layer_kwargs), \n",
    "        layers.Dense(units=10, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dropout(drop_rate),\n",
    "        layers.Dense(units=10, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        layers.Dropout(drop_rate),\n",
    "        layers.Dense(units=10, activation='relu', kernel_regularizer=l1_l2(l1_reg, l2_reg),\n",
    "                     **layer_kwargs),\n",
    "        output_layer,\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def lstm_model_sweep(l1_reg, l2_reg, drop_rate, input_len, output_layer, layer_kwargs, var):\n",
    "    input_vector_size = var.vector_size if var.embeddings else len(var.cols)\n",
    "    first_return = False\n",
    "    lstm_layers = []\n",
    "    for _ in range(var.lstm_layers - 1):\n",
    "        lstm_layers.append(layers.LSTM(units=var.units, return_sequences=first_return, **layer_kwargs)) \n",
    "        first_return = True\n",
    "              \n",
    "    first = layers.LSTM(units=var.units, return_sequences=first_return, input_shape=[None, input_vector_size],\n",
    "                        **layer_kwargs) \n",
    "    hidden_layers = []\n",
    "    for _ in range(var.layers):\n",
    "        hidden_layers.append(layers.Dropout(drop_rate))\n",
    "        hidden_layers.append(layers.Dense(units=var.units, activation='relu',\n",
    "                                          kernel_regularizer=l1_l2(l1_reg, l2_reg), **layer_kwargs)) \n",
    "    model = tf.keras.Sequential([first] + lstm_layers[::-1] + hidden_layers + [output_layer])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jameshuckle\\Dropbox\\My-Portfolio\\AlgorithmicTrading/utils\\data_util.py:410: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  train_data = ((train_data_raw[1:].T - train_data_raw[:-1,-1]) / train_data_raw[:-1,-1]).T\n",
      "C:\\Users\\Jameshuckle\\Dropbox\\My-Portfolio\\AlgorithmicTrading/utils\\data_util.py:410: RuntimeWarning: invalid value encountered in true_divide\n",
      "  train_data = ((train_data_raw[1:].T - train_data_raw[:-1,-1]) / train_data_raw[:-1,-1]).T\n",
      "C:\\Users\\Jameshuckle\\Dropbox\\My-Portfolio\\AlgorithmicTrading/utils\\data_util.py:412: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  test_data = ((test_data_raw[1:].T - test_data_raw[:-1,-1]) / test_data_raw[:-1,-1]).T\n",
      "C:\\Users\\Jameshuckle\\Dropbox\\My-Portfolio\\AlgorithmicTrading/utils\\data_util.py:412: RuntimeWarning: invalid value encountered in true_divide\n",
      "  test_data = ((test_data_raw[1:].T - test_data_raw[:-1,-1]) / test_data_raw[:-1,-1]).T\n",
      "C:\\Users\\Jameshuckle\\Dropbox\\My-Portfolio\\AlgorithmicTrading/utils\\data_util.py:270: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  y_pct_diff = (y_close_prices - x_close_prices) / x_close_prices\n",
      "C:\\Users\\Jameshuckle\\Dropbox\\My-Portfolio\\AlgorithmicTrading/utils\\data_util.py:270: RuntimeWarning: invalid value encountered in true_divide\n",
      "  y_pct_diff = (y_close_prices - x_close_prices) / x_close_prices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hit_target_and_stop: 170 # hit_neither_target_or_stop: 62\n",
      "# hit_target_and_stop: 26 # hit_neither_target_or_stop: 26\n",
      "4.193339586257935\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-284cd5729963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m### load single stock into numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     (x, y, x_test, y_test, y_pct_diff, y_test_pct_diff, train_data_raw,\n\u001b[1;32m---> 66\u001b[1;33m      test_data_raw) = create_dataset(file_name=list(loaded_files.keys())[0], var=var)\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Dropbox\\My-Portfolio\\AlgorithmicTrading/utils\\data_util.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(file_name, var)\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandardize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min_max'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    369\u001b[0m         X = self._validate_data(X, reset=first_pass,\n\u001b[0;32m    370\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                                 force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 646\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     (type_err,\n\u001b[1;32m--> 100\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m    101\u001b[0m             )\n\u001b[0;32m    102\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "class algo_variables():\n",
    "    pass\n",
    "\n",
    "var = algo_variables()\n",
    "var.window = 30 # number of training bars\n",
    "var.pca_features = False # False, 10  #must be smaller than window\n",
    "var.standardize = 'min_max' #'std', 'min_max' False\n",
    "var.norm_by_vol = False #True\n",
    "var.data_percentage_diff = 'close_diff' # False, 'close_diff', 'ohlc_diff', 'open_diff'\n",
    "var.data_percentage_diff_y = True\n",
    "var.train_split = datetime(2019,1,1) #0.9, datetime(2018,1,1)\n",
    "var.resample = None # None '1D', '4H', '1W'\n",
    "var.read_single_file = None #all_files[3] #None\n",
    "var.loaded_files = loaded_files\n",
    "\n",
    "var.num_bars = 30 # prediction horizon\n",
    "var.problem_type = 'binary' #'regression' 'binary' 'category'\n",
    "if var.problem_type == 'category':\n",
    "    var.std_thresh = 0.5 # to determine a positive, negative or flat trade\n",
    "var.dataset_type = 'stock' #'wave', 'random', 'stock', 'monte_carlo'\n",
    "var.close_only = True\n",
    "if var.close_only:\n",
    "    var.cols = ['Close'] if var.dataset_type in ['stock','monte_carlo'] else ['univariate']\n",
    "else:\n",
    "    var.cols = ['Open', 'High', 'Low', 'Close'] if var.dataset_type == 'stock' else ['univariate']\n",
    "var.multi_y = False\n",
    "\n",
    "###\n",
    "var.input_len = var.pca_features if var.pca_features else var.window\n",
    "###\n",
    "\n",
    "## target/stop binary outcomes (1 R/R) ##\n",
    "var.target_stop = True \n",
    "if var.target_stop:\n",
    "    var.num_bars = 1 # must be equal to 1!\n",
    "    var.problem_type = 'binary'\n",
    "    var.dataset_type = 'stock'\n",
    "    var.close_only = False\n",
    "    var.cols = ['Open', 'High', 'Low', 'Close']\n",
    "    var.bar_horizon = 10000 # how long to wait for stop or target hit, otherwise assign current profit\n",
    "    var.bar_size_ma = 100 # how long is moving average for bar size (used to calc stop and target)\n",
    "    var.stop_target_size = 4 # size of stop and target relative to averge bar size\n",
    "\n",
    "\n",
    "var.embeddings = False\n",
    "var.embedding_type = None #None 'light'\n",
    "if var.embeddings:\n",
    "    var.standardize = False \n",
    "    var.pca_features = False\n",
    "    var.vector_size = 200 # 200, 4\n",
    "    if var.embedding_type == 'light':\n",
    "        var.vector_size = 1\n",
    "    \n",
    "generator = False\n",
    "if generator: \n",
    "    ## save all stocks to csv and tfrecords, then load tfrecords as dataset\n",
    "    var.train_validation = 0.8 #False # Uses traning data to create test set (for validation)\n",
    "    var.batch_size = 500\n",
    "    base_path = f'C:/Users/Jameshuckle/Documents/Algo_Trading/data'\n",
    "    save_numpy_to_csv_all_files(base_path, var)\n",
    "    train_dataset = create_tfrecord_dataset(f'{base_path}/all_data_train', var)\n",
    "    test_dataset = create_tfrecord_dataset(f'{base_path}/all_data_test', var)\n",
    "else:\n",
    "    ### load single stock into numpy\n",
    "    (x, y, x_test, y_test, y_pct_diff, y_test_pct_diff, train_data_raw,\n",
    "     test_data_raw) = create_dataset(file_name=list(loaded_files.keys())[0], var=var)\n",
    "    train_dataset, test_dataset = [], []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_stop_target(test_data_raw, y_test, y_test_pct_diff, var, start=0,\n",
    "#                     end=550, vline=500,hline=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_arch(model_arch, var, sweep=False):\n",
    "    seed_value = 1\n",
    "    layer_kwargs = {'kernel_initializer':initializers.glorot_uniform(seed=seed_value),\n",
    "                    'bias_initializer':initializers.Constant(0.1),\n",
    "                   }\n",
    "\n",
    "    if var.problem_type == 'binary': \n",
    "        output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid', **layer_kwargs)\n",
    "    elif var.problem_type == 'category': \n",
    "        output_layer = tf.keras.layers.Dense(units=3, activation='softmax', **layer_kwargs)\n",
    "    else: \n",
    "        output_layer = tf.keras.layers.Dense(units=1, activation=None, **layer_kwargs)\n",
    "\n",
    "    if model_arch == 'dnn': \n",
    "        arch = dnn_model\n",
    "        if sweep: \n",
    "            arch = dnn_model_sweep\n",
    "    elif model_arch == 'conv1d': \n",
    "        arch = conv1d_model\n",
    "    elif model_arch == 'incept1d': \n",
    "        arch = inception1d_model\n",
    "    elif model_arch == 'lstm': \n",
    "        arch = lstm_model\n",
    "        if sweep: \n",
    "            arch = lstm_model_sweep\n",
    "\n",
    "    model = arch(var.l1_reg, var.l2_reg, var.drop_rate, var.input_len, output_layer,\n",
    "                       layer_kwargs, var)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, lr, var):\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    if var.problem_type == 'binary': \n",
    "        model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    elif var.problem_type == 'category':\n",
    "        model.compile(optimizer=adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        rmse = tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error', dtype=None)\n",
    "        model.compile(optimizer=adam, loss='mse', metrics=[rmse])\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def set_model_hyperparams(epochs, kwargs, plot_lr_rate, decrease_lr_rate, validation,\n",
    "                          test_dataset, generator):\n",
    "        \n",
    "    if plot_lr_rate:\n",
    "        kwargs['callbacks'].append(tf.keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch: 1e-8 * 10**(epoch / (epochs / 7))))\n",
    "\n",
    "    elif decrease_lr_rate:\n",
    "        kwargs['callbacks'].append(tf.keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch: 1e-3 / 10**(epoch / epochs))) #1e-2\n",
    "        #kwargs['callbacks'].append(tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3))\n",
    "\n",
    "    if validation:   \n",
    "        if generator:\n",
    "            kwargs['validation_data'] = test_dataset\n",
    "        else:\n",
    "            kwargs['validation_split'] = 0.7\n",
    "            \n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def reset_model_checkpoint(path=os.getcwd()):\n",
    "    total_epochs = 0\n",
    "    all_history = {}\n",
    "    # delete last saved model\n",
    "    checkpoint_path_base = f'{path}/model_checkpoints'\n",
    "    checkpoint_path_model = checkpoint_path_base + '/model.ckpt'\n",
    "    if os.path.exists(checkpoint_path_base):\n",
    "        shutil.rmtree(checkpoint_path_base)\n",
    "    return total_epochs, all_history, checkpoint_path_base, checkpoint_path_model\n",
    "\n",
    "\n",
    "def del_unneeded_checkpoints(checkpoint_path_base, all_history, metric):\n",
    "    print('deleting uneeded checkpoints')\n",
    "    epochs_idx = [\n",
    "        np.argmin(all_history['val_loss']) + 1,\n",
    "        np.argmax(all_history['val_loss']) + 1,\n",
    "        np.argmin(all_history[f'val_{metric}']) + 1, \n",
    "        np.argmax(all_history[f'val_{metric}']) + 1,\n",
    "        len(all_history['val_loss']),\n",
    "    ]\n",
    "    for cp_file in os.listdir(checkpoint_path_base):\n",
    "        if 'model_epoch-' in cp_file:\n",
    "            epoch = int(cp_file.split('-')[1].split('.')[0])\n",
    "            if epoch not in epochs_idx:\n",
    "                os.remove(f'{checkpoint_path_base}/{cp_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "###\n",
    "model_arch = 'dnn' # 'dnn','lstm','conv1d','incept1d'\n",
    "var.l1_reg = 1e-7 #1e-9 #1e-6\n",
    "var.l2_reg = 1e-7 #1e-8 #1e-5\n",
    "var.drop_rate = 0 #0.2 #0.1 #0.2\n",
    "###\n",
    "sweep = True\n",
    "if sweep:\n",
    "    var.layers = 5\n",
    "    var.units = 100\n",
    "    var.lstm_layers = 1\n",
    "\n",
    "model = get_model_arch(model_arch, var, sweep)\n",
    "(total_epochs, all_history, checkpoint_path_base,\n",
    " checkpoint_path_model) = reset_model_checkpoint(\n",
    "    path='B:Algo_Trading/model_checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# load model to keep continuity of epochs. To create new model run cell above.\n",
    "if os.path.exists(checkpoint_path_model):\n",
    "    print('loading model')\n",
    "    model = tf.keras.models.load_model(checkpoint_path_model)\n",
    "\n",
    "model = compile_model(model=model, lr=1e-4, var=var)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "plot_lr_rate = False\n",
    "decrease_lr_rate = False\n",
    "validation = True\n",
    "epochs = 100\n",
    "\n",
    "checkpoint_path_cb = checkpoint_path_base+'/model_epoch-{epoch}.ckpt'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path_cb,\n",
    "                                                 save_best_only=False,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_accuracy',\n",
    "                                                 mode='max', verbose=0)\n",
    "kwargs = {'verbose':2, 'epochs':epochs, 'initial_epoch':total_epochs, 'shuffle':False,\n",
    "          'callbacks':[cp_callback]}\n",
    "kwargs = set_model_hyperparams(epochs, kwargs, plot_lr_rate, decrease_lr_rate,\n",
    "                               validation, test_dataset, generator)\n",
    "\n",
    "if generator:\n",
    "    ### Parellelize loading\n",
    "    history = model.fit(x=train_dataset, **kwargs)\n",
    "else:\n",
    "    batch_size = 100\n",
    "    history = model.fit(x, y, batch_size=batch_size, **kwargs)\n",
    "                        \n",
    "model.save(checkpoint_path_model)\n",
    "print('\\n---------------')\n",
    "total_epochs = epochs\n",
    "print('\\ntotal_epochs',total_epochs)\n",
    "gc.collect()\n",
    "\n",
    "for key, value in history.history.items():\n",
    "    all_history.setdefault(key, [])\n",
    "    all_history[key] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "end_epoch = -1\n",
    "\n",
    "if plot_lr_rate:\n",
    "    plt.plot(all_history['lr'][start_epoch:end_epoch],\n",
    "             all_history['loss'][start_epoch:end_epoch])\n",
    "    plt.xscale('log')\n",
    "    lowest_loss_idx = np.argmin(all_history['loss'])\n",
    "    print('best lr:',all_history['lr'][lowest_loss_idx])\n",
    "elif decrease_lr_rate:\n",
    "    plt.plot(all_history['lr'][start_epoch:end_epoch],\n",
    "             all_history['loss'][start_epoch:end_epoch])\n",
    "    plt.title('learning rate')\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(max(all_history['lr'][start_epoch:end_epoch]),\n",
    "             min(all_history['lr'][start_epoch:end_epoch]))\n",
    "else:\n",
    "    plt.plot(all_history['loss'][start_epoch:end_epoch])\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15,15))\n",
    "\n",
    "metric = 'root_mean_squared_error' if var.problem_type == 'regression' else 'accuracy'\n",
    "epoch_show_from = int(epochs * 0)\n",
    "# metric\n",
    "ax[0][0].plot(all_history[metric][epoch_show_from:])\n",
    "ax[0][0].legend([f'training {metric}'])\n",
    "ax[0][0].set_title(metric)\n",
    "# loss\n",
    "ax[0][1].plot(all_history['loss'][epoch_show_from:])\n",
    "ax[0][1].legend([f'training loss'])\n",
    "ax[0][1].set_title('loss')\n",
    "\n",
    "if validation:\n",
    "    # metric\n",
    "    ax[1][0].plot(all_history[f'val_{metric}'][epoch_show_from:])\n",
    "    ax[1][0].legend([f'validation {metric}'])\n",
    "    ax[2][0].plot(all_history[metric][epoch_show_from:])\n",
    "    ax[2][0].plot(all_history[f'val_{metric}'][epoch_show_from:])\n",
    "    ax[2][0].legend([f'training {metric}',f'validation {metric}'])\n",
    "\n",
    "    # loss\n",
    "    ax[1][1].plot(all_history[f'val_loss'][epoch_show_from:])\n",
    "    ax[1][1].legend([f'validation loss'])\n",
    "    ax[2][1].plot(all_history['loss'][epoch_show_from:])\n",
    "    ax[2][1].plot(all_history[f'val_loss'][epoch_show_from:])\n",
    "    ax[2][1].legend([f'training loss',f'validation loss'])\n",
    "else:\n",
    "    ax[2][0].legend(['train'])\n",
    "    ax[2][1].legend(['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "#del_unneeded_checkpoints(checkpoint_path_base, all_history, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_epoch(metric, man_epoch_idx, man_val_metric, checkpoint_path_base, all_history, model):\n",
    "    if man_epoch_idx:\n",
    "        print('load weights from epoch', man_epoch_idx)\n",
    "        checkpoint_path = f'{checkpoint_path_base}/model_epoch-{man_epoch_idx}.ckpt'\n",
    "    else:\n",
    "        val_metric = f'val_{metric}' if not man_val_metric else man_val_metric\n",
    "        print(f'Loaded weights from best {val_metric}')\n",
    "        if val_metric in ['val_root_mean_squared_error', 'val_loss']:\n",
    "            best_idx = np.argmin(all_history[val_metric])\n",
    "            best_acc = round((all_history[val_metric][best_idx]), 4)\n",
    "        else:\n",
    "            best_idx = np.argmax(all_history[val_metric])\n",
    "            best_acc = round((all_history[val_metric][best_idx]), 4)\n",
    "        print(f'best {val_metric}: {best_acc} | epoch={best_idx}')\n",
    "        checkpoint_path = f'{checkpoint_path_base}/model_epoch-{best_idx + 1}.ckpt'\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "man_epoch_idx = 0 #Set to 0 or False to choose best accuracy, otherwise choose epoch to load\n",
    "man_val_metric = False #False 'val_loss'\n",
    "\n",
    "model = explore_epoch(metric, man_epoch_idx, man_val_metric, checkpoint_path_base, all_history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise Exception('stop at this cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_data_set(review_set, var):\n",
    "    if review_set == 'train':\n",
    "        review_x_data = var.x \n",
    "        review_y_data = var.y\n",
    "        review_y_pct_diff = var.y_pct_diff\n",
    "    elif review_set == 'test':\n",
    "        review_x_data = var.x_test\n",
    "        review_y_data = var.y_test\n",
    "        review_y_pct_diff = var.y_test_pct_diff\n",
    "    elif review_set == 'all':\n",
    "        review_x_data = np.concatenate([var.x, var.x_test], axis=0)\n",
    "        review_y_data = np.concatenate([var.y, var.y_test], axis=0)\n",
    "        review_y_pct_diff = np.concatenate([var.y_pct_diff, var.y_test_pct_diff], axis=0)\n",
    "    return review_x_data, review_y_data, review_y_pct_diff\n",
    "\n",
    "def format_predictions_tanh(review_x_data, model, var):\n",
    "    raw_predictions = model.predict(review_x_data)\n",
    "    if var.problem_type == 'binary': \n",
    "        predictions_tanh = np.where(raw_predictions.flatten() < 0.5, -1, 1) \n",
    "    elif var.problem_type == 'category':\n",
    "        class_idx = np.argmax(raw_predictions, axis=1)\n",
    "        predictions_tanh = class_idx - 1\n",
    "        raw_predictions[:, 0] = -raw_predictions[:, 0] \n",
    "        raw_predictions[:, 1] = 0\n",
    "        raw_predictions = raw_predictions[range(raw_predictions.shape[0]),class_idx]\n",
    "    else:\n",
    "        predictions_tanh = raw_predictions.flatten()\n",
    "    return raw_predictions, predictions_tanh\n",
    "\n",
    "def cut_off_start_review_data_for_prediction(review_x_data, review_set, var):\n",
    "    # cut off start of raw data until first prediction candle\n",
    "    if review_set in 'train':\n",
    "        review_data_raw = var.train_data_raw[-len(review_x_data):]\n",
    "    elif review_set == 'test':\n",
    "        review_data_raw = var.test_data_raw[-len(review_x_data):]\n",
    "    elif review_set == 'all':\n",
    "        review_data_raw = np.concatenate([var.train_data_raw[:-len(review_x_data)],\n",
    "                                          var.test_data_raw[:-len(review_x_data)]], axis=0)\n",
    "    review_data_raw = review_data_raw[:,-1]   \n",
    "    \n",
    "    return review_data_raw\n",
    "\n",
    "def see_predictions(review_set, model, var): \n",
    "    review_x_data, review_y_data, review_y_pct_diff = create_review_data_set(review_set, var)\n",
    "    raw_predictions, predictions_tanh = format_predictions_tanh(review_x_data, model, var)\n",
    "    review_data_raw = cut_off_start_review_data_for_prediction(review_x_data, review_set, var)\n",
    "    return (review_data_raw, review_x_data, review_y_data, review_y_pct_diff, raw_predictions,\n",
    "           predictions_tanh)\n",
    "\n",
    "def plot_preds(bars_to_plot, predictions_tanh, review_data_raw):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(predictions_tanh[:bars_to_plot], c='orange')\n",
    "    ax1.legend(['prediction'])\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(review_data_raw[:bars_to_plot], c='blue')\n",
    "    #ax2.plot(review_ data_raw[var.input_len + var.num_bars: var.input_len + var.num_bars + bars_to_plot],\n",
    "    #         c='blue')\n",
    "    plt.show()\n",
    "    smallest, biggest = predictions_tanh.min(), predictions_tanh.max()\n",
    "    print(smallest,'to', biggest, 'range:', biggest - smallest )\n",
    "    \n",
    "def calc_returns(review_data_raw, predictions_tanh, raw_predictions, review_y_pct_diff, var, pip_fees=1):\n",
    "    returns = pd.DataFrame(pd.Series(review_data_raw, name='test_data_raw'))\n",
    "    returns['y_pct_diff'] = review_y_pct_diff\n",
    "    returns['predictions_tanh'] = pd.Series(predictions_tanh.flatten())\n",
    "    returns['raw_predictions'] = pd.Series(raw_predictions.flatten())\n",
    "    \n",
    "    if var.problem_type == 'regression':\n",
    "        returns['predictions_tanh'] = returns['raw_predictions'] - returns['test_data_raw']\n",
    "        \n",
    "    longs = returns.query('predictions_tanh > 0')\n",
    "    flat = returns.query('predictions_tanh == 0')\n",
    "    shorts = returns.query('predictions_tanh < 0')\n",
    "    returns.loc[longs.index, 'profit'] = returns.loc[longs.index,'y_pct_diff'] - (0.0001 * pip_fees)\n",
    "    returns.loc[flat.index, 'profit'] = 0\n",
    "    returns.loc[shorts.index, 'profit'] = -returns.loc[shorts.index,'y_pct_diff'] - (0.0001 * pip_fees)\n",
    "    returns['returns'] = returns['profit'].cumsum()\n",
    "    avg_profit = returns['profit'].mean()\n",
    "    print(f'avg_profit ({pip_fees} pip fees)',avg_profit)\n",
    "    print(f'best possible profit ({pip_fees} pip fees)',returns['y_pct_diff'].abs().mean())\n",
    "        \n",
    "    return avg_profit, returns\n",
    "\n",
    "def plot_returns():\n",
    "    returns['returns'].plot()\n",
    "    plt.show()\n",
    "    ##\n",
    "    ax = returns['profit'].hist()\n",
    "    ax.axvline(0, c='k')\n",
    "    ax.axvline(avg_profit, c='lightgreen')\n",
    "    plt.show()\n",
    "    \n",
    "def populate_var_with_dataset(file_name, var):\n",
    "    (x, y, x_test, y_test, y_pct_diff, y_test_pct_diff, train_data_raw,\n",
    "     test_data_raw) = create_dataset(file_name=file_name, var=var)\n",
    "    \n",
    "    var.x, var.y, var.x_test, var.y_test = x, y, x_test, y_test\n",
    "    var.y_pct_diff, var.y_test_pct_diff = y_pct_diff, y_test_pct_diff\n",
    "    var.train_data_raw, var.test_data_raw = train_data_raw, test_data_raw\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "pip_fees = 1\n",
    "review_set = 'test' #'test' 'train' 'all'\n",
    "dataset_type = 'stock'\n",
    "\n",
    "file_name = list(loaded_files.keys())[0]\n",
    "print(file_name)\n",
    "var = populate_var_with_dataset(file_name, var)\n",
    "\n",
    "(review_data_raw, review_x_data, review_y_data, review_y_pct_diff, raw_predictions,\n",
    " predictions_tanh) = see_predictions(review_set, model, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "bars_to_plot = 1000\n",
    "plot_preds(bars_to_plot, predictions_tanh, review_data_raw)\n",
    "\n",
    "raw_predictions, predictions_tanh = format_predictions_tanh(review_x_data, model, var)\n",
    "preds_df = pd.DataFrame(pd.Series(raw_predictions.flatten(), name='preds'))\n",
    "preds_df['y'] = review_y_data\n",
    "preds_df.plot.scatter('preds','y')\n",
    "print('---')\n",
    "print('long:', (preds_df['preds'] > 0.5).sum(), 'short:', (preds_df['preds'] < 0.5).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "review_data_raw = review_data_raw.flatten()\n",
    "avg_profit, returns = calc_returns(review_data_raw, predictions_tanh, raw_predictions,\n",
    "                                   review_y_pct_diff, var, pip_fees=pip_fees)\n",
    "plot_returns()\n",
    "\n",
    "plt.plot(review_data_raw)\n",
    "plt.title('raw data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise Exception('stop at this cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def out_of_sample_results(loaded_files, pip_fees, review_set, model, var):\n",
    "    all_returns = []\n",
    "    all_raw = []\n",
    "    for file_name in list(loaded_files.keys())[:]:\n",
    "        gc.collect()\n",
    "        print(file_name)\n",
    "        var = populate_var_with_dataset(file_name, var)\n",
    "        if len(var.x) <= 1:\n",
    "            continue\n",
    "        (review_data_raw, review_x_data, review_y_data, review_y_pct_diff, raw_predictions,\n",
    "         predictions_tanh) = see_predictions(review_set, model, var)\n",
    "        review_data_raw = review_data_raw.flatten()\n",
    "        avg_profit, returns = calc_returns(review_data_raw, predictions_tanh, raw_predictions, review_y_pct_diff,\n",
    "                                           var, pip_fees=pip_fees)\n",
    "        num_trades = len(returns)\n",
    "        print('no. trades:', num_trades)\n",
    "        if var.resample:\n",
    "            index_data = loaded_files[file_name].resample(var.resample).agg({'Open':'first',\n",
    "                                                        'High':'max','Low':'min','Close':'last'})\n",
    "        else:\n",
    "            index_data = loaded_files[file_name]\n",
    "            \n",
    "        if review_set in ['train', 'all']:\n",
    "            returns.index = index_data[:num_trades].index\n",
    "        elif review_set == 'test':\n",
    "            returns.index = index_data[-num_trades:].index\n",
    "        returns['profit'].name=file_name\n",
    "        returns['y_pct_diff'].name=file_name\n",
    "        all_returns.append(returns['profit'])\n",
    "        all_raw.append(returns['y_pct_diff'])\n",
    "\n",
    "    return all_returns, all_raw\n",
    "\n",
    "def drop_outliers(all_returns_final):\n",
    "    suspect_profits = all_returns_final.max(axis=0).sort_values(ascending=False)\n",
    "    suspect_high_stocks = list(suspect_profits[suspect_profits > 1].index) # larger than 100% gain on any trade\n",
    "    print('suspect high stock trade:', suspect_profits[suspect_profits > 1].round(2).to_dict())\n",
    "    suspect_losses = all_returns_final.min(axis=0).sort_values(ascending=True)\n",
    "    suspect_low_stocks = list(suspect_losses[suspect_losses < -0.5].index) # larger than -50% loss on any trade\n",
    "    print('suspect low stock trades:', suspect_losses[suspect_losses < -0.5].round(2).to_dict())\n",
    "    suspect_stocks = suspect_high_stocks + suspect_low_stocks\n",
    "    all_returns_final.drop(suspect_stocks, axis='columns', inplace=True)  \n",
    "    return all_returns_final, suspect_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "pip_fees = 1\n",
    "review_set = 'test' #'test' 'train' ' all'\n",
    "\n",
    "all_returns, all_raw = out_of_sample_results(loaded_files, pip_fees, review_set, model, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "all_returns_final = pd.concat(all_returns, axis=1)\n",
    "all_returns_final, suspect_stocks = drop_outliers(all_returns_final)\n",
    "\n",
    "print(f'averge profit (after {pip_fees} pip fees):',np.nanmean(all_returns_final))\n",
    "all_returns_final['profit'] = all_returns_final.sum(axis=1)\n",
    "all_returns_final['returns'] = all_returns_final['profit'].cumsum()\n",
    "all_returns_final['returns'].plot(title='all returns (time scaled)')\n",
    "plt.show()\n",
    "all_returns_final['returns'].reset_index(drop=True).plot(title='all returns (no time)')\n",
    "plt.show()\n",
    "\n",
    "daily_pct_change = all_returns_final['profit'].resample('1D').sum()\n",
    "romad = calc_romad(daily_pct_change, filter_large_trades=False, yearly_agg=np.median, plot=True)\n",
    "\n",
    "all_raw_final = pd.concat(all_raw, axis=1)\n",
    "all_raw_final.drop(suspect_stocks, axis='columns', inplace=True) \n",
    "raw_daily_pct_change = all_raw_final.sum(axis=1).resample('1D').sum()\n",
    "romad_raw = calc_romad(raw_daily_pct_change, filter_large_trades=False, yearly_agg=np.median, plot=False)\n",
    "print('romad_raw:', romad_raw)\n",
    "raw_equity = raw_daily_pct_change.cumsum().reset_index(drop=True)\n",
    "raw_equity.plot(title='raw combined')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "file_name = list(loaded_files.keys())[3]\n",
    "print(file_name)\n",
    "\n",
    "var = populate_var_with_dataset(file_name, var)\n",
    "(review_data_raw, review_x_data, review_y_data, review_y_pct_diff, raw_predictions,\n",
    " predictions_tanh) = see_predictions(review_set, model, var)\n",
    "review_data_raw = review_data_raw.flatten()\n",
    "\n",
    "rmse_func = lambda y, y_hat: mean_squared_error(y, y_hat)**0.5\n",
    "scorer = rmse_func if var.problem_type == 'regression' else accuracy_score\n",
    "\n",
    "if review_set == 'train':\n",
    "    y_data = var.y\n",
    "elif review_set == 'test':\n",
    "    y_data = var.y_test\n",
    "elif review_set == 'all':\n",
    "    y_data = np.concatenate([var.y, var.y_test], axis=0)\n",
    "\n",
    "if var.problem_type == 'category':\n",
    "    logit_predictions = predictions_tanh + 1\n",
    "else:\n",
    "    logit_predictions = np.where(raw_predictions < 0.5, 0, 1) \n",
    "\n",
    "dumb_pred = scorer(y_data, np.zeros(len(y_data)))\n",
    "print('Dumb:', dumb_pred)\n",
    "model_score = scorer(y_data, logit_predictions.flatten())\n",
    "print(f'{metric}:',model_score)\n",
    "performance = dumb_pred - model_score if var.problem_type == 'regression' else model_score - dumb_pred\n",
    "print('Performance:', performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "avg_profit, returns = calc_returns(review_data_raw, predictions_tanh, raw_predictions,\n",
    "                                   review_y_pct_diff, var, pip_fees=pip_fees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "num_trades = len(returns)\n",
    "print('no. trades:', num_trades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "win = (returns['profit'] >= 0).sum()\n",
    "loss = (returns['profit'] < 0).sum()\n",
    "win_per = win / (win + loss)\n",
    "print('win percetange',round(win_per,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "raise Exception('stop at this cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "def bin_returns(returns):\n",
    "    returns['pred_bins'] = pd.cut(returns['raw_predictions'], bins=20)\n",
    "    grouped_returns = returns.groupby('pred_bins')\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=3, sharey=True, figsize=(15,8))\n",
    "    grouped_returns.min()['profit'].plot.barh(title='min', ax=ax[0,0])\n",
    "    grouped_returns.max()['profit'].plot.barh(title='max', ax=ax[0,1])\n",
    "    grouped_returns.mean()['profit'].plot.barh(title='mean', ax=ax[1,0])\n",
    "    grouped_returns.sum()['profit'].plot.barh(title='sum', ax=ax[1,1])\n",
    "    grouped_returns.count()['profit'].plot.barh(title='count', ax=ax[1,2])\n",
    "    plt.show()\n",
    "    return grouped_returns\n",
    "\n",
    "\n",
    "def filter_returns(returns, problem_type, upper_max=1, upper=0.75, lower=0.25, lower_min=0):\n",
    "    if problem_type == 'binary': \n",
    "        filtered_returns = returns.query('((raw_predictions > @upper) & (raw_predictions < @upper_max)) | \\\n",
    "                                          ((raw_predictions > @lower_min) & (raw_predictions < @lower))').copy()\n",
    "    elif problem_type == 'category': \n",
    "        filtered_returns = returns.query('((raw_predictions > @upper) & (raw_predictions < @upper_max)) | \\\n",
    "                                          ((raw_predictions > @lower_min) & (raw_predictions < @lower))').copy()\n",
    "    else:\n",
    "        filtered_returns = returns.query('(raw_predictions > @upper) | (raw_predictions < @lower)').copy()\n",
    "\n",
    "    filtered_returns['returns'] = filtered_returns['profit'].cumsum()\n",
    "    \n",
    "    avg_filtered_profit = filtered_returns['profit'].mean()\n",
    "    print('avg_filtered_profit:',avg_filtered_profit)\n",
    "    avg_filtered_profit_pips = avg_filtered_profit* 10 ** 4\n",
    "    print('avg_filtered_profit_pips:', avg_filtered_profit_pips)\n",
    "    num_samples = len(filtered_returns)\n",
    "    print('no. trades:',num_samples)\n",
    "#     add_datetime_index(filtered_returns.copy()) \n",
    "#     sharpe = calc_sharpe(filtered_returns['profit'])\n",
    "#     print('sharpe:', sharpe)\n",
    "    return filtered_returns, avg_filtered_profit_pips, num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "pip_fees = 1 # each instrument\n",
    "review_set = 'test' #'test' 'train' 'all'\n",
    "\n",
    "upper_max = 1 #0.65\n",
    "upper = 0.6\n",
    "lower = 0.4\n",
    "lower_min = 0 #0.393\n",
    "\n",
    "total_avg_weight_pips = []\n",
    "for file_name in loaded_files.keys():\n",
    "    gc.collect()\n",
    "    print('\\n-----',file_name)\n",
    "    var = populate_var_with_dataset(file_name, var)    \n",
    "    plt.plot(var.test_data_raw[:,-1])\n",
    "    plt.title(file_name)\n",
    "    plt.show()\n",
    "    (review_data_raw, review_x_data, review_y_data, review_y_pct_diff, raw_predictions,\n",
    "     predictions_tanh) = see_predictions(review_set, model, var)\n",
    "    review_data_raw = review_data_raw.flatten()\n",
    "    avg_profit, returns = calc_returns(review_data_raw, predictions_tanh, raw_predictions, review_y_pct_diff,\n",
    "                                       var, pip_fees=1)\n",
    "    returns['returns'].plot(title='returns')\n",
    "    grouped_returns = bin_returns(returns)\n",
    "    filtered_returns, avg_pips, samples = filter_returns(returns, var.problem_type, upper_max,\n",
    "                                                         upper, lower, lower_min)\n",
    "    if len(filtered_returns) > 0:\n",
    "        filtered_returns['returns'].plot()\n",
    "        plt.show()\n",
    "        \n",
    "    total_avg_weight_pips.append([samples, avg_pips])\n",
    "    \n",
    "total_avg_weight_pips = np.array(total_avg_weight_pips)\n",
    "tot_avg_pips = ((total_avg_weight_pips[:,0] * total_avg_weight_pips[:,1]).sum() \n",
    "                / total_avg_weight_pips[:,0].sum())\n",
    "print('\\n---\\n total average pips per filtered trade:', round(tot_avg_pips, 4))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
